{
 "cells": [
  {
   "cell_type": "code",
   "id": "4cc3fa03",
   "metadata": {},
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature\n",
    "import earthpy.spatial as es\n",
    "import geopandas as gpd\n",
    "import matplotlib.gridspec\n",
    "import matplotlib.lines\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotnine as p9\n",
    "import pyproj\n",
    "import rasterio as rio\n",
    "import scipy.stats\n",
    "import shapely.geometry\n",
    "import sklearn.metrics\n",
    "import xarray as xr\n",
    "from PIL import Image\n",
    "from matplotlib_scalebar.scalebar import ScaleBar\n",
    "from scipy.ndimage import gaussian_filter"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "with plt.style.context((\"tableau-colorblind10\")):\n",
    "    color_list = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    print(color_list)"
   ],
   "id": "53b89f523812b563",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f6909c9d",
   "metadata": {},
   "source": [
    "def my_highlight_cols(c, col_to_index: dict, cols_to_ignore=None):\n",
    "    \"\"\"\n",
    "        Highlight a certain for column c using the map col_to_index, which maps which lines should be made bold.\n",
    "    \"\"\"\n",
    "    styles = [''] * len(c)\n",
    "    col_name = c.name[0] if isinstance(c.name, tuple) else c.name\n",
    "    if cols_to_ignore is not None and col_name in cols_to_ignore:\n",
    "        return styles\n",
    "\n",
    "    # make bold the indicated lines\n",
    "    styles[col_to_index[col_name]] = 'font-weight:bold'\n",
    "    return styles"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.rcParams['mathtext.fontset'] = 'custom'\n",
    "plt.rcParams['mathtext.rm'] = 'cmr10'\n",
    "plt.rcParams['mathtext.cal'] = 'cmr10'\n",
    "matplotlib.rcParams[\"font.family\"] = \"cmr10\"\n",
    "matplotlib.rcParams[\"font.size\"] = \"12\"\n",
    "plt.rcParams['axes.formatter.use_mathtext'] = True"
   ],
   "id": "61a32eee42a74685",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Paths",
   "id": "8e298092bbb00552"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "results_root_dir = Path('../data/external/_experiments/s2_alps_plus')\n",
    "model_root_dir = results_root_dir / 'unet'\n",
    "model_version = 'version_0'\n",
    "\n",
    "# main directory for plots\n",
    "plots_dir = Path(f'../data/external/scratch/plots')\n",
    "plots_dir.mkdir(parents=True, exist_ok=True)"
   ],
   "id": "f57d6aca25009415",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Read the inventory outlines",
   "id": "a4a799ce8a2c2a6a"
  },
  {
   "cell_type": "code",
   "id": "7b5b9566",
   "metadata": {},
   "source": [
    "outlines_fp = '../data/outlines/paul_et_al_2020/c3s_gi_rgi11_s2_2015_v2.shp'\n",
    "gl_df = gpd.read_file(outlines_fp)\n",
    "gl_df['date_inv'] = gl_df.date_inv.apply(pd.to_datetime)\n",
    "gl_df['area_inv'] = gl_df.area_km2\n",
    "gl_df['year_inv'] = gl_df.date_inv.apply(lambda d: d.year)\n",
    "\n",
    "# number of glaciers and total area \n",
    "print(f\"n = {len(gl_df)}; area = {gl_df.area_inv.sum():.1f} km2\")\n",
    "\n",
    "# compute how many glaciers are above 0.1 km2 and the percentage of their areas\n",
    "gl_sdf = gl_df[gl_df.area_inv >= 0.1]\n",
    "print(\n",
    "    f\"n = {len(gl_sdf)};\"\n",
    "    f\" area = {gl_sdf.area_inv.sum():.1f} km2\"\n",
    "    f\" ({gl_sdf.area_inv.sum() / gl_df.area_inv.sum() * 100:.1f}%)\"\n",
    ")\n",
    "gl_sdf"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Table with the number of glaciers and their total area separated by year",
   "id": "92e623de86890853"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "table_df = gl_df.groupby('year_inv').agg({'GLACIER_NR': 'count', 'area_inv': 'sum'}).round(1).reset_index()\n",
    "table_df.GLACIER_NR = table_df.GLACIER_NR.apply(lambda x: f\"{x} ({x / len(gl_df) * 100:.1f}\\\\%)\")\n",
    "table_df.area_inv = table_df.area_inv.apply(lambda x: f\"{x} km$^2$ ({x / gl_df.area_inv.sum() * 100:.1f}\\\\%)\")\n",
    "table_df.columns = ['Year', 'Count', 'Area']\n",
    "\n",
    "# add a line with the sum\n",
    "table_df.loc[len(table_df.index)] = ['All', len(gl_df), f\"{gl_df.area_inv.sum():.1f} km$^2$\"]\n",
    "\n",
    "display()\n",
    "\n",
    "out = table_df.style.hide(axis=\"index\").to_latex(\n",
    "    hrules=True,\n",
    "    convert_css=True\n",
    ")\n",
    "bold_lines = {c: -1 for c in table_df.columns}\n",
    "\n",
    "out = table_df.style.hide(axis=\"index\").apply(my_highlight_cols, col_to_index=bold_lines, axis=0).to_latex(\n",
    "    hrules=True,\n",
    "    convert_css=True,\n",
    "    column_format=''.join(['c'] * len(table_df.columns))\n",
    ")\n",
    "\n",
    "for x in out.splitlines():\n",
    "    print(f\"\\t{x}\")\n",
    "\n",
    "# out = out.replace('_', '\\\\_')\n",
    "# out = out.replace('font\\\\-weightbold', 'font-weightbold')\n",
    "# print(out)"
   ],
   "id": "77c9d33723a8e8e3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Table with the data filtering (QC) results\n",
   "id": "f5e5ae388fbcec1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_dates_final = pd.read_csv('../data/inv_images_qc/final_dates.csv')\n",
    "display(df_dates_final)\n",
    "\n",
    "table_df_totals = {'Step': [], 'Count': [], 'Area': []}\n",
    "\n",
    "# add the initial stats\n",
    "table_df_totals['Step'].append('Before any filtering')\n",
    "table_df_totals['Count'].append(len(gl_df))\n",
    "table_df_totals['Area'].append(f\"{gl_df.area_inv.sum():.1f} km$^2$\")\n",
    "\n",
    "# area filtering\n",
    "table_df_totals['Step'].append(r'After filtering by area ($\\geq$ 0.1 km$^2$)')\n",
    "table_df_totals['Count'].append(f\"{len(gl_sdf)} ({len(gl_sdf) / len(gl_df) * 100:.1f}%)\")\n",
    "table_df_totals['Area'].append(\n",
    "    f\"{gl_sdf.area_inv.sum():.1f} km$^2$ ({gl_sdf.area_inv.sum() / gl_df.area_inv.sum() * 100:.1f}%)\")\n",
    "\n",
    "# after manual quality check\n",
    "table_df_totals['Step'].append('After manual quality check')\n",
    "table_df_totals['Count'].append(f\"{len(df_dates_final)} ({len(df_dates_final) / len(gl_df) * 100:.1f}%)\")\n",
    "_df = gl_df[gl_df.entry_id.isin(df_dates_final.entry_id)]\n",
    "table_df_totals['Area'].append(\n",
    "    f\"{_df.area_inv.sum():.1f} km$^2$ ({_df.area_inv.sum() / gl_df.area_inv.sum() * 100:.1f}%)\")\n",
    "\n",
    "table_df_totals = pd.DataFrame(table_df_totals)\n",
    "display(table_df_totals)\n",
    "\n",
    "out = table_df_totals.style.hide(axis=\"index\").to_latex(\n",
    "    hrules=True,\n",
    "    convert_css=True,\n",
    "    column_format=''.join(['l'] * len(table_df_totals.columns))\n",
    ")\n",
    "\n",
    "out = out.replace('%', '\\\\%')\n",
    "for x in out.splitlines():\n",
    "    print(f\"\\t{x}\")\n",
    "\n",
    "# out = out.replace('_', '\\\\_')\n",
    "# out = out.replace('font\\\\-weightbold', 'font-weightbold')\n",
    "# print(out)"
   ],
   "id": "cc532bc6287dfa19",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Table with the glacier statistics of the test folds for each cross-validation iteration\n",
   "id": "9b38179bd8e11b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dir_splits = Path('../data/external/wd/s2_alps_plus/cv_split_outlines/')\n",
    "\n",
    "table_df_totals = {'Subregion': [], 'Lon range': [], '#Glaciers': [], 'Area': []}\n",
    "for i_split in range(1, 1 + 5):\n",
    "    fp = dir_splits / f\"split_{i_split}\" / f\"fold_test.shp\"\n",
    "    print(f\"Reading the split df from {fp}\")\n",
    "    gdf = gpd.read_file(fp)\n",
    "    gdf = gdf.to_crs(epsg=4326)\n",
    "    print(f\"split_{i_split}: n = {len(gdf)}; area = {gdf.area_km2.sum():.1f} km2\")\n",
    "    table_df_totals['Subregion'].append(f\"$R_{i_split}$\")\n",
    "    # add leading spaces to make the table look better\n",
    "    s1 = '\\\\ ' if gdf.total_bounds[0] < 10 else ''\n",
    "    s2 = '\\\\ ' if gdf.total_bounds[2] < 10 else ''\n",
    "    table_df_totals['Lon range'].append(f\"{s1}{gdf.total_bounds[0]:.1f}° - {s2}{gdf.total_bounds[2]:.1f}° E\")\n",
    "    table_df_totals['#Glaciers'].append(len(gdf))\n",
    "    table_df_totals['Area'].append(gdf.area_km2.sum())\n",
    "    # fp_split = dir_splits / f\"split_{i_split}\"\n",
    "table_df_totals = pd.DataFrame(table_df_totals)\n",
    "print(table_df_totals.sum())\n",
    "\n",
    "# add the percentages for the number of glaciers and area\n",
    "table_df_totals['#Glaciers'] = table_df_totals['#Glaciers'].apply(lambda x: f\"{x} ({x / len(gl_sdf) * 100:.1f}%)\")\n",
    "table_df_totals['Area'] = table_df_totals['Area'].apply(\n",
    "    lambda x: f\"{x:.1f} km$^2$ ({x / gl_sdf.area_inv.sum() * 100:.1f}%)\")\n",
    "\n",
    "display(table_df_totals)\n",
    "\n",
    "out = (\n",
    "    table_df_totals.style.hide(axis='index')\n",
    "    .to_latex(\n",
    "        hrules=True,\n",
    "        convert_css=True,\n",
    "        column_format=''.join(['c'] * len(table_df_totals.columns)),\n",
    "    )\n",
    ")\n",
    "\n",
    "# escape Latex characters\n",
    "out = out.replace('%', '\\\\%')\n",
    "out = out.replace('#', '\\\\#')\n",
    "out = out.replace('_', '\\\\_').replace(r'R\\_', r'R_')\n",
    "\n",
    "# rename the first column & make it into two lie\n",
    "\n",
    "for x in out.splitlines():\n",
    "    print(f\"\\t{x}\")\n",
    "\n",
    "print()"
   ],
   "id": "86cbff403892e647",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Tables with performance metrics\n",
    "1. by subregion, using standard ML metrics (for the inventory) - median per glacier\n",
    "2. by year & glacier size (both the inventory and 2023) - median & sum per glacier class\n",
    "\n",
    "Metrics are obtained by running the script `eval.sh`)"
   ],
   "id": "d6988948788b0ab1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ds_name = 's2_alps_plus/inv'\n",
    "ds_fold = 'test'\n",
    "seed = 'all'\n",
    "buffer_m = 0  # we don't allow any mistake\n",
    "\n",
    "table_df = {'subregion': [], 'accuracy': [], 'iou': [], 'precision': [], 'recall': [], 'f1': []}\n",
    "\n",
    "df_all = []\n",
    "for i_split in range(1, 5 + 1):\n",
    "    fp = f'{model_root_dir}/split_{i_split}/seed_{seed}/{model_version}/output/stats_calib/{ds_name}/s_{ds_fold}/stats_calib.csv'\n",
    "    df = pd.read_csv(fp)\n",
    "\n",
    "    # dummy columns so it doesn't fail with buffer_m = 0\n",
    "    df['area_non_g_b0'] = 0\n",
    "    df['area_non_g_pred_b0'] = 0\n",
    "\n",
    "    df['i_split'] = i_split\n",
    "\n",
    "    df_all.append(df)\n",
    "\n",
    "df_all = pd.concat(df_all)\n",
    "\n",
    "for i_split in [1, 2, 3, 4, 5, None]:\n",
    "    df = df_all.copy()\n",
    "    if i_split is not None:\n",
    "        df = df[df.i_split == i_split]\n",
    "\n",
    "    s = f\"b{'-' if buffer_m != 0 else ''}{buffer_m}\"\n",
    "    p = df[f\"area_{s}\"]\n",
    "    tp = df[f\"area_pred_{s}\"]\n",
    "    fn = p - tp\n",
    "\n",
    "    # for FP we always look up to 50m\n",
    "    fp = df['area_non_g_pred_b50'] - df[f\"area_non_g_pred_b{buffer_m}\"]\n",
    "    n = df['area_non_g_b50'] - df[f\"area_non_g_b{buffer_m}\"]\n",
    "    tn = n - fp\n",
    "\n",
    "    # compute the 5 metrics\n",
    "    accuracy = (tp + tn) / (tp + fp + fn + tn) * 100\n",
    "    iou = tp / (tp + fp + fn) * 100\n",
    "    precision = tp / (tp + fp) * 100\n",
    "    recall = tp / (tp + fn) * 100\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "    table_df['subregion'].append(rf'$R_{i_split}$' if i_split is not None else 'All')\n",
    "    for c, v in zip(\n",
    "            ('accuracy', 'iou', 'precision', 'recall', 'f1'),\n",
    "            (accuracy, iou, precision, recall, f1)\n",
    "    ):\n",
    "        # table_df[c].append(rf\"\\makecell{{{v.mean():.3f} \\\\ ± \\\\ {v.std():.3f}}}\")\n",
    "        prefix = '' if v.std() >= 10 else r'\\ '\n",
    "        table_df[c].append(f\"{v.mean():.1f} ± {prefix}{v.std():.1f}\")\n",
    "\n",
    "table_df = pd.DataFrame(table_df)\n",
    "display(table_df)\n",
    "\n",
    "out = (\n",
    "    table_df.style.hide(axis='index')\n",
    "    .to_latex(\n",
    "        hrules=True,\n",
    "        convert_css=True,\n",
    "        column_format=''.join(['c'] * len(table_df.columns))\n",
    "    )\n",
    ")\n",
    "\n",
    "for x in out.splitlines():\n",
    "    if 'all' in x:\n",
    "        print('\\t\\\\midrule')\n",
    "    print(f\"\\t{x}\")"
   ],
   "id": "fba5c7f82e7bf5ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# table_df['n_glaciers'].append(len(sdf))\n",
    "# table_df['area_p'].append(area_p)\n",
    "# table_df['area_pred_tp'].append(area_pred_tp)\n",
    "# table_df['area_pred_fn'].append(area_pred_fn)\n",
    "# table_df['area_n'].append(area_n)\n",
    "# table_df['area_pred_fp'].append(area_pred_fp)\n",
    "#\n",
    "# _df_all = (df.rename(\n",
    "#     columns={\n",
    "#         'split': 'Subregion',\n",
    "#         'BinaryAccuracy': 'Accuracy',\n",
    "#         'BinaryJaccardIndex': 'IOU',\n",
    "#         'BinaryPrecision': 'Precision',\n",
    "#         'BinaryRecall': 'Recall',\n",
    "#         'BinaryF1Score': 'F1'}\n",
    "# ).drop(\n",
    "#     columns=['entry_id', 'Recall_debris', 'loss', 'seed']\n",
    "# ))\n",
    "#\n",
    "# _df_all.subregion = _df_all.subregion.apply(lambda s: s.replace('split', 'R'))\n",
    "\n",
    "# table_df_avg = _df_all.groupby('subregion').mean(numeric_only=True).reset_index()\n",
    "# table_df_std = _df_all.groupby(['subregion', 'fp']).std(numeric_only=True).reset_index().drop(\n",
    "#     columns='fp').groupby(\n",
    "#     'subregion').agg(lambda x: np.mean(x ** 2) ** 0.5)\n",
    "#\n",
    "# display(table_df_avg)\n",
    "# display(table_df_std)\n",
    "#\n",
    "# # make a table showing the avg ± std\n",
    "# table_df = table_df_avg.copy()\n",
    "# for c in table_df_std:\n",
    "#     table_df[c] = [\n",
    "#         # rf\"\\makecell{{{v_mean:.3f} \\\\ ± \\\\ {v_std:.4f}}}\"\n",
    "#         f\"{v_mean:.3f} ± {v_std:.3f}\"\n",
    "#         for (v_mean, v_std) in zip(table_df_avg[c].values, table_df_std[c].values)\n",
    "#     ]\n",
    "#\n",
    "# out = (\n",
    "#     table_df.style.hide(axis='index')\n",
    "#     .to_latex(\n",
    "#         hrules=True,\n",
    "#         convert_css=True,\n",
    "#         column_format=''.join(['c'] * len(table_df.columns))\n",
    "#     )\n",
    "# )\n",
    "# out = re.sub(r'(R_\\d+)', r'$\\1$', out)  # covert to subscript\n",
    "# out = out.replace('#', '\\\\#')\n",
    "#\n",
    "# # rename the subregion column to make it narrower\n",
    "# out = out.replace('subregion', 'R')\n",
    "# out = out.replace('JaccardIndex', 'IOU')\n",
    "#\n",
    "# for x in out.splitlines():\n",
    "#     print(f\"\\t{x}\")"
   ],
   "id": "aef1a0e905adb9a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "1cd9dd6e8268cedb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ds_name = 's2_alps_plus'\n",
    "model_name = 'unet'\n",
    "model_v = 'version_0'  # all inputs\n",
    "subdir = 'inv'\n",
    "# subdir = '2023'\n",
    "\n",
    "buffer_m = 0\n",
    "print(f\"buffer_m = {buffer_m}\")\n",
    "\n",
    "fp_results = results_root_dir / model_name / 'stats_all_splits' / f'df_stats_calib_all_{ds_name}_{subdir}_{model_v}_ensemble.csv'\n",
    "print(f\"Reading the processed results from {fp_results}\")\n",
    "df = pd.read_csv(fp_results)\n",
    "\n",
    "area_thrs = [0.1, 0.2, 0.5, 1, 2, 5, 10, 20, np.inf]  # based on Paul et al. 2020\n",
    "\n",
    "table_df = {k: [] for k in ['area_class', 'n_glaciers', 'P', 'TP', 'TPR', 'FN', 'FNR', 'N', 'FP', 'FPR', 'rel_unc']}\n",
    "\n",
    "for i in range(len(area_thrs)):\n",
    "    if i < len(area_thrs) - 1:\n",
    "        min_area = area_thrs[i]\n",
    "        max_area = area_thrs[i + 1]\n",
    "        area_class = f\"[{min_area}, {max_area})\" if max_area != np.inf else f\">= {min_area}\"\n",
    "    else:\n",
    "        # use all of them\n",
    "        min_area = 0\n",
    "        max_area = np.inf\n",
    "        area_class = 'All'\n",
    "\n",
    "    idx = (min_area <= df.area_inv) & (df.area_inv < max_area)\n",
    "    sdf = df[idx]\n",
    "\n",
    "    # prepare all the numbers per glacier which will then be combined into avg & totals\n",
    "    s = f\"b{'-' if buffer_m != 0 else ''}{buffer_m}\"\n",
    "    p_all = sdf[f\"area_{s}\"]\n",
    "    tp_all = sdf[f\"area_pred_{s}\"]\n",
    "    fn_all = p_all - tp_all\n",
    "\n",
    "    # for FP we always look up to 50m\n",
    "    n_all = (sdf['area_non_g_b50'] - sdf[f\"area_non_g_b{buffer_m}\"])\n",
    "    fp_all = (sdf['area_non_g_pred_b50'] - sdf[f\"area_non_g_pred_b{buffer_m}\"])\n",
    "\n",
    "    table_df['area_class'].append(area_class)\n",
    "    table_df['n_glaciers'].append(len(sdf))\n",
    "    table_df['P'].append(p_all.sum())\n",
    "    table_df['TP'].append(tp_all.sum())\n",
    "    table_df['TPR'].append(tp_all.sum() / p_all.sum() * 100)\n",
    "    table_df['FN'].append(fn_all.sum())\n",
    "    table_df['FNR'].append(fn_all.sum() / p_all.sum() * 100)\n",
    "    table_df['N'].append(n_all.sum())\n",
    "    table_df['FP'].append(fp_all.sum())\n",
    "    table_df['FPR'].append(fp_all.sum() / n_all.sum() * 100)\n",
    "    table_df['rel_unc'].append((fn_all.sum() + fp_all.sum()) / p_all.sum() * 100)\n",
    "    # table_df['rel_unc'].append((fn_all.sum() + fp_all.sum()) / tp_all.sum() * 100) # v2\n",
    "\n",
    "    # table_df['area_class'].append(' ')\n",
    "    # table_df['n_glaciers'].append(' ')\n",
    "    # table_df['P'].append(p_all.mean())\n",
    "    # table_df['TP'].append(tp_all.mean())\n",
    "    # table_df['TPR'].append((tp_all / p_all).mean() * 100)\n",
    "    # table_df['FN'].append(fn_all.mean())\n",
    "    # table_df['FNR'].append((fn_all / p_all).mean() * 100)\n",
    "    # table_df['N'].append(n_all.mean())\n",
    "    # table_df['FP'].append(fp_all.mean())\n",
    "    # table_df['FPR'].append((fp_all / n_all).mean() * 100)\n",
    "    # table_df['rel_unc'].append(((fn_all + fp_all) / tp_all).median() * 100)\n",
    "\n",
    "# table_df_s = table_df.copy()\n",
    "# for c in table_df.columns[2:]:\n",
    "#     v_s = []\n",
    "#     for i in range(len(table_df)):\n",
    "#         v_s.append(f\"{table_df[c][i].round(i%2 + 1)}\")\n",
    "#     table_df_s[c] = v_s\n",
    "# table_df_s\n",
    "\n",
    "table_df = pd.DataFrame(table_df)\n",
    "table_df['area_class'] = table_df['area_class'].apply(lambda s: f\"${s}$\".replace(r'>=', r'\\geq') if s != ' ' else ' ')\n",
    "\n",
    "nc = len(table_df.columns)\n",
    "format_dict = {\n",
    "    c: v for c, v in zip(\n",
    "        table_df.columns,\n",
    "        ['{}', '{:3.0f}'] + [f\"{{:{len(str(int(table_df[c].iloc[:-1].max().round()))) + 2}.1f}}\" for c in\n",
    "                             table_df.columns[2:]]\n",
    "    )}\n",
    "print([f\"{{:{len(str(int(table_df[c].max().round()))) + 2}.1f}}\" for c in table_df.columns[2:]])\n",
    "out = (\n",
    "    table_df.style.format(format_dict).hide(axis='index')\n",
    "    .to_latex(\n",
    "        hrules=True,\n",
    "        convert_css=True,\n",
    "        # column_format='cc' + 'r' * (nc - 2),\n",
    "        column_format='c' * nc,\n",
    "    )\n",
    ")\n",
    "out = out.replace('  ', ' \\\\ ')\n",
    "\n",
    "for x in out.splitlines():\n",
    "    if 'All' in x:\n",
    "        print('\\t\\\\midrule')\n",
    "    print(f\"\\t{x}\")"
   ],
   "id": "774766796753f724",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Table with the regional-level results for the best model vs. 1) the one without dh/dt and 2) the band-ratio models",
   "id": "56ed5965bc6c890d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_all_models = []\n",
    "for model_name, model_v in zip(\n",
    "        ('unet', 'unet', 'band_ratio_regional', 'band_ratio_glacier-wide'),\n",
    "        ('version_0', 'version_1', 'version_0', 'version_0')\n",
    "):\n",
    "    label = 'ensemble' if model_name == 'unet' else 'individual'\n",
    "    stats_name = 'stats_calib' if model_name == 'unet' else 'stats'\n",
    "    fp_results_inv = results_root_dir / model_name / 'stats_all_splits' / f'df_{stats_name}_all_s2_alps_plus_inv_{model_v}_{label}.csv'\n",
    "    fp_results_2023 = results_root_dir / model_name / 'stats_all_splits' / f'df_{stats_name}_all_s2_alps_plus_2023_{model_v}_{label}.csv'\n",
    "    print(f\"Reading the processed results from\\n\\t1. {fp_results_inv}\\n\\t2. {fp_results_2023}\")\n",
    "    df_crt_model = pd.concat([pd.read_csv(fp_results_inv), pd.read_csv(fp_results_2023)])\n",
    "    df_crt_model['model'] = f\"{model_name}_{model_v}\"\n",
    "    df_all_models.append(df_crt_model)\n",
    "\n",
    "df_all_models = pd.concat(df_all_models)\n",
    "\n",
    "# check if each glacier has the exactly four predictions for each year\n",
    "assert (df_all_models.groupby('entry_id').size() == 4 * 2).all()\n",
    "df_all_models"
   ],
   "id": "6bcba12e87b7d15a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# compute the recall and the FP rate using the total predicted areas (so not per glacier)\n",
    "# use the 20m buffer (i.e. use the area_pred column which should already include the buffer)\n",
    "assert np.isclose(df_all_models.area_pred, df_all_models.area_pred_b20).all()\n",
    "\n",
    "df_stats = df_all_models[df_all_models.is_inv_year].groupby('model', sort=False)[\n",
    "    ['area_inv', 'area_pred_b0', 'area_pred', 'area_non_g_b20_50', 'area_non_g_pred_b20_50']].sum()\n",
    "display(df_stats)\n",
    "print(f\"#glaciers = {len(set(df_all_models.entry_id))}\")\n",
    "print(f\"total area inv = {df_stats.area_inv.iloc[0]:.1f} km2\")\n",
    "print(f\"total area b20_50 = {df_stats.area_non_g_b20_50.iloc[0]:.1f} km2\")\n",
    "df_stats['recall'] = df_stats.area_pred_b0 / df_stats.area_inv * 100\n",
    "df_stats['fp_rate_b20_50'] = df_stats.area_non_g_pred_b20_50 / df_stats.area_non_g_b20_50 * 100\n",
    "\n",
    "# add the debris recall for the glaciers in 'CH' & at least 1% debris\n",
    "idx = df_all_models.is_inv_year & (df_all_models.Country == 'CH') & (df_all_models.area_debris_p >= 0.01)\n",
    "df_stats_debris_ch = df_all_models[idx].groupby('model', sort=False)[\n",
    "    ['area_inv', 'area_debris', 'area_debris_pred_b0']].sum()\n",
    "print(f\"total area inv (CH & with debris) = {df_stats_debris_ch.area_inv.iloc[0]:.2f} km2\")\n",
    "print(f\"total area debris (CH & with debris) = {df_stats_debris_ch.area_debris.iloc[0]:.2f} km2\")\n",
    "df_stats['recall_debris'] = df_stats_debris_ch.area_debris_pred_b0 / df_stats_debris_ch.area_debris * 100\n",
    "\n",
    "# create another index level for the columns & keep only the relevant columns\n",
    "df_stats = pd.DataFrame({('inventory', c): df_stats[c] for c in ['recall', 'recall_debris', 'fp_rate_b20_50']})\n",
    "\n",
    "# add the total predicted area for the inventory year and then 2023)\n",
    "df_stats[('inventory', 'total_area_pred')] = df_all_models[df_all_models.is_inv_year].groupby('model',\n",
    "                                                                                              sort=False).area_pred.sum()\n",
    "df_stats[('2023', 'total_area_pred')] = df_all_models[~df_all_models.is_inv_year].groupby('model',\n",
    "                                                                                          sort=False).area_pred.sum()\n",
    "\n",
    "# compute the rate of change per year\n",
    "df_area_diffs_per_year = pd.DataFrame({\n",
    "    'model': df_all_models[df_all_models.is_inv_year].model.values,\n",
    "    'area_pred_t0': df_all_models[df_all_models.is_inv_year].area_pred.values,\n",
    "    'area_pred_t1': df_all_models[~df_all_models.is_inv_year].area_pred.values,\n",
    "    'num_years_diff': df_all_models[~df_all_models.is_inv_year].year.values - df_all_models[\n",
    "        df_all_models.is_inv_year].year.values\n",
    "})\n",
    "df_area_diffs_per_year['area_rates_per_year'] = \\\n",
    "    (df_area_diffs_per_year.area_pred_t1 - df_area_diffs_per_year.area_pred_t0) / df_area_diffs_per_year.num_years_diff\n",
    "df_area_diffs_per_year_stats = df_area_diffs_per_year.groupby('model', sort=False).sum()\n",
    "df_area_diffs_per_year_stats['area_rates_per_year_prc'] = \\\n",
    "    df_area_diffs_per_year_stats.area_rates_per_year / df_area_diffs_per_year_stats.area_pred_t0 * 100\n",
    "# overall_change_rate_per_year = area_diffs_per_year.sum() / area_pred_t0.sum()\n",
    "# overall_change_rate_per_year\n",
    "df_stats[('', 'area_rates_per_year_prc')] = df_area_diffs_per_year_stats['area_rates_per_year_prc']\n",
    "\n",
    "df_stats\n",
    "# df_stats['total_area_pred']"
   ],
   "id": "1f1d40fbe6b85150",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Note: the columns will be renamed in Latex; copy only the values\n",
    "table_df_totals = df_stats.copy()\n",
    "\n",
    "# switch the third column with the first one\n",
    "cols = table_df_totals.columns.tolist()\n",
    "table_df_totals = table_df_totals.reindex(columns=[cols[3]] + cols[:3] + cols[4:])\n",
    "\n",
    "# add the method names in the first column\n",
    "table_df_totals.insert(loc=0, column='method',\n",
    "                       value=['DL4GAM', 'DL4GAM (no dh/dt)', 'band-ratio (v1)', 'band-ratio (v2)'])\n",
    "display(table_df_totals)\n",
    "\n",
    "format_dict = {c: v for c, v in\n",
    "               zip(table_df_totals.columns, ['{}', '{:.1f}', '{:.1f}', '{:.1f}', '{:.1f}', '{:.1f}', '{:.2f}'])}\n",
    "out = (\n",
    "    table_df_totals.style.format(format_dict).hide(axis='index')\n",
    "    .to_latex(\n",
    "        hrules=True,\n",
    "        convert_css=True,\n",
    "        column_format=''.join(['c'] * len(table_df_totals.columns)),\n",
    "    )\n",
    ")\n",
    "\n",
    "for x in out.splitlines():\n",
    "    # add a line in between the methods\n",
    "    if x.startswith('band-ratio (v1)'):\n",
    "        print(\"\\t\\\\midrule\")\n",
    "    if x.startswith('DL4GAM (no dh/dt)'):\n",
    "        print(\"\\t\\\\midrule\")\n",
    "    print(f\"\\t{x}\")\n",
    "\n",
    "print()"
   ],
   "id": "d66af45cd7c5581d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Figures",
   "id": "12b9337f3ad47f95"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### For one glacier plot: 1) the ensemble average 2) the ensemble std and 3) the image with the prediction & the buffer on top",
   "id": "e890edd6e694d672"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gl_df_pred_avg = gpd.read_file(\n",
    "    model_root_dir / 'gdf_all_splits/seed_all/version_0/s2_alps_plus/inv/inv_preds_calib.shp')\n",
    "gl_df_pred_lb = gpd.read_file(\n",
    "    model_root_dir / 'gdf_all_splits/seed_all/version_0/s2_alps_plus/inv/inv_preds_calib_low.shp')\n",
    "gl_df_pred_ub = gpd.read_file(\n",
    "    model_root_dir / 'gdf_all_splits/seed_all/version_0/s2_alps_plus/inv/inv_preds_calib_high.shp')"
   ],
   "id": "769ed5abace3eea1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "entry_id_to_fp_pred = {\n",
    "    'g_2164': model_root_dir / 'split_2/seed_all/version_0/output/preds_calib/s2_alps_plus/inv/s_test/g_2164/20160906.nc',\n",
    "    'g_4165': model_root_dir / 'split_1/seed_all/version_0/output/preds_calib/s2_alps_plus/inv/s_test/g_4165/20150826.nc',\n",
    "    'g_0208': model_root_dir / 'split_5/seed_all/version_0/output/preds_calib/s2_alps_plus/inv/s_test/g_0208/20150829.nc'\n",
    "}\n",
    "\n",
    "entry_id = 'g_2164'\n",
    "# entry_id = 'g_4165'\n",
    "# entry_id = 'g_0208'\n",
    "fp_pred = entry_id_to_fp_pred[entry_id]\n",
    "\n",
    "# read the raster data\n",
    "fp_list = list(Path(f'../data/external/wd/s2_alps_plus/inv/glacier_wide/{entry_id}').glob('*.nc'))\n",
    "assert len(fp_list) == 1\n",
    "fp = fp_list[0]\n",
    "nc = xr.open_dataset(fp, decode_coords='all')\n",
    "\n",
    "# read the ensemble prediction\n",
    "nc_pred = xr.open_dataset(fp_pred, decode_coords='all')\n",
    "display(nc_pred)\n",
    "\n",
    "# add the image data to the prediction dataset\n",
    "nc_pred['band_data'] = nc.band_data\n",
    "\n",
    "# keep only a 500m buffer around the glacier from the prediction raster\n",
    "g_bbox = shapely.geometry.box(*gl_df[gl_df.entry_id == entry_id].iloc[0].geometry.bounds)\n",
    "g_buff = g_bbox.buffer(500)\n",
    "g_buff_bbox = shapely.geometry.box(*g_buff.bounds)\n",
    "nc_pred = nc_pred.rio.clip([g_buff_bbox])\n",
    "nc_pred"
   ],
   "id": "6138f4dbaa947d21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# project the outlines to the local UTM of the raster\n",
    "gl_df_proj = gl_df.to_crs(nc.rio.crs)\n",
    "gl_df_pred_avg_proj = gl_df_pred_avg.to_crs(nc.rio.crs)\n",
    "gl_df_pred_lb_proj = gl_df_pred_lb.to_crs(nc.rio.crs)\n",
    "gl_df_pred_ub_proj = gl_df_pred_ub.to_crs(nc.rio.crs)\n",
    "\n",
    "# increase the font size to compensate for the larger figure size\n",
    "font_size = 16\n",
    "original_font_size = matplotlib.rcParams[\"font.size\"]\n",
    "matplotlib.rcParams[\"font.size\"] = str(font_size)\n",
    "\n",
    "# dirty hacks to adjust the height ratio of the colorbar and the space between the subplots\n",
    "is_first = False  # show the titles only for the first glacier\n",
    "is_last = False  # show the colorbar only for the last glacier\n",
    "if entry_id == 'g_2164':\n",
    "    gs = matplotlib.gridspec.GridSpec(1, 4)\n",
    "    is_first = True\n",
    "elif entry_id == 'g_4165':\n",
    "    gs = matplotlib.gridspec.GridSpec(1, 4)\n",
    "elif entry_id == 'g_0208':\n",
    "    h_ratio_cb = 0.06\n",
    "    is_last = True\n",
    "    gs = matplotlib.gridspec.GridSpec(2, 4, height_ratios=[1, 0.03])\n",
    "else:\n",
    "    raise ValueError(f\"entry_id = {entry_id}\")\n",
    "\n",
    "# plot the RGB image\n",
    "fig_w = 18\n",
    "f = nc_pred.pred.shape[0] / nc_pred.pred.shape[1]\n",
    "fig_h = fig_w / 3 * f\n",
    "fig = plt.figure(dpi=250, figsize=(fig_w, fig_h))\n",
    "\n",
    "img1 = nc_pred.band_data.isel(band=[nc_pred.band_data.long_name.index(b) for b in ['R', 'G', 'B']]).values.copy()\n",
    "img1 = img1.transpose(1, 2, 0)\n",
    "\n",
    "# clip and scale the image\n",
    "vmin = np.quantile(img1, 0.01)\n",
    "vmax = np.quantile(img1, 0.99)\n",
    "img1 = np.clip(img1, vmin, vmax)\n",
    "img1 = (img1 - vmin) / (vmax - vmin)\n",
    "\n",
    "extent = [nc_pred.x.min(), nc_pred.x.max(), nc_pred.y.min(), nc_pred.y.max()]\n",
    "\n",
    "### suplot 1 - the RGB image with the glacier outline\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "ax1.imshow(img1, extent=extent)\n",
    "# plt.grid(alpha=0.1, linestyle='--')\n",
    "ax1.grid(False)\n",
    "ax1.xaxis.set_label_position('top')\n",
    "ax1.xaxis.tick_top()\n",
    "if is_first:\n",
    "    ax1.set_xlabel('Easting [m]')\n",
    "ax1.set_ylabel('Northing [m]')\n",
    "ax1.yaxis.get_offset_text().set_x(-0.2)\n",
    "\n",
    "gl_df_proj[gl_df_proj.entry_id == entry_id].plot(\n",
    "    ax=plt.gca(), facecolor='none', edgecolor=color_list[0], linewidth=1.25, label='inventory'\n",
    ")\n",
    "# build the legend (manually; default fails with UserWarning: Legend does not support handles for PatchCollection instances)\n",
    "if is_first:\n",
    "    legend_handles = [matplotlib.lines.Line2D([0], [0], color=color_list[0], label=f'inventory')]\n",
    "    ax1.legend(handles=legend_handles, loc='upper right', prop={'size': font_size - 1})\n",
    "\n",
    "# add the scalebar\n",
    "# (https://pypi.org/project/matplotlib-scalebar/: \"Set dx to 1.0 if the axes image has already been calibrated by setting its extent\")\n",
    "ax1.add_artist(\n",
    "    ScaleBar(\n",
    "        dx=1.0,\n",
    "        length_fraction=0.2,\n",
    "        font_properties={'size': font_size - 1},\n",
    "        frameon=True,\n",
    "        scale_loc='right',\n",
    "        location='lower left'\n",
    "    )\n",
    ")\n",
    "\n",
    "### subplot 2 - the ensemble average\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "img = nc_pred.pred.values\n",
    "im = ax2.imshow(img, cmap='hot', vmin=0, vmax=1, zorder=1)\n",
    "if is_first:\n",
    "    ax2.set_title('Ensemble \\naverage')\n",
    "ax2.axis('off')\n",
    "if is_last:\n",
    "    cax = fig.add_subplot(gs[1, 1])\n",
    "    fig.colorbar(im, cax=cax, orientation='horizontal')\n",
    "\n",
    "### subplot 3 - the ensemble std\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "img = nc_pred.pred_std.values\n",
    "im = ax3.imshow(img, cmap='viridis', zorder=1, vmin=0, vmax=0.5)\n",
    "if is_first:\n",
    "    ax3.set_title('Ensemble \\nstddev')\n",
    "ax3.axis('off')\n",
    "if is_last:\n",
    "    cat = fig.add_subplot(gs[1, 2])\n",
    "    fig.colorbar(im, cax=cat, orientation='horizontal')\n",
    "\n",
    "# subplot 4 - the prediction and the buffer, on top of the image\n",
    "ax4 = fig.add_subplot(gs[0, 3])\n",
    "ax4.imshow(img1, extent=extent)\n",
    "ax4.axis('off')\n",
    "\n",
    "# plot the buffer\n",
    "s_lb = gl_df_pred_lb_proj[gl_df_pred_lb_proj.entry_id == entry_id]\n",
    "s_ub = gl_df_pred_ub_proj[gl_df_pred_ub_proj.entry_id == entry_id]\n",
    "s_ub.difference(s_lb).plot(ax=ax4, edgecolor=color_list[7], facecolor=color_list[7], linewidth=0.75)\n",
    "\n",
    "# plot the predicted glacier outlines and the corresponding uncertainty buffer\n",
    "s = gl_df_pred_avg_proj[gl_df_pred_avg_proj.entry_id == entry_id]\n",
    "s.plot(ax=ax4, facecolor='none', edgecolor=color_list[1], linewidth=0.75)\n",
    "\n",
    "# build the legend (manually; default fails with UserWarning: Legend does not support handles for PatchCollection instances)\n",
    "if is_first:\n",
    "    legend_handles = [\n",
    "        matplotlib.lines.Line2D([0], [0], color=color_list[1], label=f'average pred.'),\n",
    "        matplotlib.lines.Line2D([0], [0], color=color_list[7], label=f'uncertainty'),\n",
    "    ]\n",
    "    # Add the custom legend to the plot\n",
    "    ax4.legend(handles=legend_handles, loc='upper right', prop={'size': font_size - 1})\n",
    "\n",
    "# gl_df_pred_lb[gl_df_pred_lb.entry_id == entry_id].plot(ax=plt.gca(), facecolor='none', edgecolor=color_list[1],\n",
    "#                                                    linewidth=0.75)\n",
    "# gl_df_pred_ub[gl_df_pred_ub.entry_id == entry_id].plot(ax=plt.gca(), facecolor='none', edgecolor=color_list[2],\n",
    "#                                                    linewidth=0.75)\n",
    "\n",
    "area_inv = gl_df_proj[gl_df_proj.entry_id == entry_id].area_inv.values[0]\n",
    "print(f\"area_inv = {area_inv:.2f}\")\n",
    "area_lb = s_lb.area.values[0] / 1e6\n",
    "area_ub = s_ub.area.values[0] / 1e6\n",
    "area_avg = s.area.values[0] / 1e6\n",
    "ax4.set_title(\n",
    "    f\"A$_{{\\\\mu}}$ = {area_avg:.3f} km$^{{2}}$\\n\"\n",
    "    f\"68.2% CI = [{area_lb:.3f}, {area_ub:.3f}] km$^{{2}}$\",\n",
    "    fontsize=font_size - 1\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "pos = ax1.get_position()\n",
    "fig.subplots_adjust(wspace=0.075)\n",
    "if is_last:\n",
    "    fig.subplots_adjust(hspace=-0.2)\n",
    "\n",
    "# add the North arrow (on the first image only)\n",
    "if is_first:\n",
    "    im_north = Image.open(plots_dir / 'north.png')\n",
    "    f = 0.01 * fig.bbox.xmax / im_north.size[0]\n",
    "    im_north = im_north.resize((int(f * im_north.size[0]), int(f * im_north.size[1])))\n",
    "    im_north = np.array(im_north).astype(float) / 255\n",
    "    im_north[:, :, :3] = 1 - im_north[:, :, :3]  # invert the colors\n",
    "    pos = ax1.get_position()\n",
    "    fig.figimage(im_north, xo=pos.x1 * fig.bbox.width * 0.925, yo=pos.y0 * fig.bbox.height * 0.2)\n",
    "    # for ax in axes:\n",
    "#     ax.set_aspect('equal')\n",
    "\n",
    "plt.savefig(plots_dir / f'ensemble_{entry_id}.pdf', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# reset the font size\n",
    "matplotlib.rcParams[\"font.size\"] = original_font_size"
   ],
   "id": "d90275e63f7ee2a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Figure with all the inputs and the predictions for a single glacier (will be assembled externally)",
   "id": "fd92464d67199f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "entry_id = 'g_0498'\n",
    "\n",
    "# read the raster data (for the inventory year and 2023)\n",
    "fp_list = list(Path(f'../data/external/wd/s2_alps_plus/inv/glacier_wide/{entry_id}').glob('*.nc'))\n",
    "assert len(fp_list) == 1\n",
    "fp1 = fp_list[0]\n",
    "\n",
    "fp_list = list(Path(f'../data/external/wd/s2_alps_plus/2023/glacier_wide/{entry_id}').glob('*.nc'))\n",
    "assert len(fp_list) == 1\n",
    "fp2 = fp_list[0]\n",
    "\n",
    "nc1 = xr.open_dataset(fp1, decode_coords='all')\n",
    "nc2 = xr.open_dataset(fp2, decode_coords='all')\n",
    "\n",
    "# read the ensemble predictions\n",
    "gl_df_pred1 = gpd.read_file(model_root_dir / 'gdf_all_splits/seed_all/version_0/s2_alps_plus/inv/inv_preds_calib.shp')\n",
    "gl_df_pred2 = gpd.read_file(model_root_dir / 'gdf_all_splits/seed_all/version_0/s2_alps_plus/2023/2023_preds_calib.shp')\n",
    "\n",
    "# reproject to the local UTM\n",
    "gl_df_proj = gl_df.to_crs(nc1.rio.crs)\n",
    "gl_df_pred1 = gl_df_pred1.to_crs(nc1.rio.crs)\n",
    "gl_df_pred2 = gl_df_pred2.to_crs(nc2.rio.crs)"
   ],
   "id": "2f17cfd62756de1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# extract the bands needed to compute the NDSI, NDVI and NDWI indices, separately for each year\n",
    "# NDSI = (Green - SWIR) / (Green + SWIR)\n",
    "# NDVI = (NIR - Red) / (NIR + Red)\n",
    "# NDWI = (Green - NIR) / (Green + NIR)\n",
    "band_data = nc1.band_data.values\n",
    "swir1 = band_data[nc1.band_data.long_name.index('SWIR')]\n",
    "r1 = band_data[nc1.band_data.long_name.index('R')]\n",
    "g1 = band_data[nc1.band_data.long_name.index('G')]\n",
    "nir1 = band_data[nc1.band_data.long_name.index('NIR')]\n",
    "y1 = nc1.fn[:4]\n",
    "\n",
    "band_data = nc2.band_data.values\n",
    "swir2 = band_data[nc2.band_data.long_name.index('SWIR')]\n",
    "r2 = band_data[nc2.band_data.long_name.index('R')]\n",
    "g2 = band_data[nc2.band_data.long_name.index('G')]\n",
    "nir2 = band_data[nc2.band_data.long_name.index('NIR')]\n",
    "y2 = nc2.fn[:4]"
   ],
   "id": "5a47e074f30593e2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for y in (y1, y2):\n",
    "    for show_outlines in (False, True):\n",
    "        nc = (nc1 if y == y1 else nc2)\n",
    "\n",
    "        # plot the RGB image\n",
    "        fig = plt.figure(dpi=200, figsize=(4, 4))\n",
    "        img1 = nc.band_data.isel(band=[nc.band_data.long_name.index(b) for b in ['R', 'G', 'B']]).values.copy()\n",
    "        img1 = img1.transpose(1, 2, 0)\n",
    "\n",
    "        # clip and scale the image\n",
    "        vmin = np.quantile(img1, 0.01)\n",
    "        vmax = np.quantile(img1, 0.99)\n",
    "        img1 = np.clip(img1, vmin, vmax)\n",
    "        img1 = (img1 - vmin) / (vmax - vmin)\n",
    "\n",
    "        extent = [nc.x.min(), nc.x.max(), nc.y.min(), nc.y.max()]\n",
    "        plt.imshow(img1, extent=extent)\n",
    "        # plt.grid(alpha=0.1, linestyle='--')\n",
    "        plt.grid(False)\n",
    "        plt.xlabel('Easting [m]')\n",
    "        plt.ylabel('Northing [m]')\n",
    "\n",
    "        if show_outlines:\n",
    "            # plot the glacier outlines\n",
    "            gl_df_proj[gl_df_proj.entry_id == entry_id].plot(\n",
    "                ax=plt.gca(), facecolor='none', edgecolor=color_list[0], linewidth=2\n",
    "            )\n",
    "\n",
    "            # plot the ensemble prediction\n",
    "            gl_df_pred = gl_df_pred1 if y == y1 else gl_df_pred2\n",
    "            gl_df_pred[gl_df_pred.entry_id == entry_id].plot(\n",
    "                ax=plt.gca(), facecolor='none', edgecolor=color_list[1], linewidth=1.5\n",
    "            )\n",
    "\n",
    "            # build the legend (manually; default fails with UserWarning: Legend does not support handles for PatchCollection instances)\n",
    "            legend_handles = [\n",
    "                matplotlib.lines.Line2D([0], [0], color=color_list[0], label=f'Inventory ({y1})'),\n",
    "                matplotlib.lines.Line2D([0], [0], color=color_list[1], label=f'Prediction ({y})'),\n",
    "            ]\n",
    "            # Add the custom legend to the plot\n",
    "            plt.legend(handles=legend_handles, loc='upper right')\n",
    "\n",
    "            # draw a rectangle to mark the snow covered area (for the inventory year)\n",
    "            if y == y1:\n",
    "                rect_h, rect_w = 600, 600\n",
    "                rect_coords = (365400 - rect_w // 2, 5041600 - rect_h // 2, 365400 + rect_w // 2, 5041600 + rect_h // 2)\n",
    "                rect = plt.Rectangle(\n",
    "                    xy=tuple(rect_coords[:2]), width=rect_w, height=rect_h, color=color_list[3], fill=False,\n",
    "                    linewidth=1.5, linestyle='--'\n",
    "                )\n",
    "                plt.gca().add_artist(rect)\n",
    "                img_nc_1_cp = img1.copy()\n",
    "\n",
    "            # disable the axis labels\n",
    "            plt.axis('off')\n",
    "\n",
    "        # add the scalebar & North (for the images without outlines)\n",
    "        if not show_outlines:\n",
    "            # add the scalebar\n",
    "            # (https://pypi.org/project/matplotlib-scalebar/: \"Set dx to 1.0 if the axes image has already been calibrated by setting its extent\")\n",
    "            plt.gca().add_artist(\n",
    "                ScaleBar(\n",
    "                    dx=1.0,\n",
    "                    length_fraction=0.2,\n",
    "                    font_properties={'size': 12},\n",
    "                    frameon=True,\n",
    "                    scale_loc='right',\n",
    "                    location='upper right'\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # # move the y-axis labels to the right\n",
    "            plt.gca().yaxis.get_offset_text().set_x(-0.2)\n",
    "            plt.gca().yaxis.get_offset_text().set_size(9)\n",
    "\n",
    "            # move the x-axis labels to the top\n",
    "            plt.gca().xaxis.set_label_position('top')\n",
    "            plt.gca().xaxis.tick_top()\n",
    "\n",
    "            # add the North arrow\n",
    "            im_north = Image.open(plots_dir / 'north.png')\n",
    "            f = 0.075 * fig.bbox.ymax / im_north.size[1]\n",
    "            im_north = im_north.resize((int(f * im_north.size[0]), int(f * im_north.size[1])))\n",
    "            im_north = np.array(im_north).astype(float) / 255\n",
    "            im_north[:, :, :3] = 1 - im_north[:, :, :3]  # invert the colors\n",
    "            fig.figimage(im_north, xo=fig.bbox.xmax * 0.875, yo=fig.bbox.ymax * 0.055)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plots_dir / f'img_{y}_outlines_{show_outlines}.png', bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "# print the areas\n",
    "print(f\"Inventory area: {gl_df[gl_df.entry_id == entry_id].area_inv.values[0]:.2f} km2\")\n",
    "area_pred_inv = gl_df_pred1[gl_df_pred1.entry_id == entry_id].area.values[0] / 1e6\n",
    "print(f\"Predicted area (inv): {area_pred_inv:.2f} km2\")\n",
    "area_pred_2023 = gl_df_pred2[gl_df_pred2.entry_id == entry_id].area.values[0] / 1e6\n",
    "print(f\"Predicted area (2023): {area_pred_2023:.2f} km2\")\n",
    "print(f\"Annual area change rate: {(area_pred_2023 - area_pred_inv) / area_pred_inv / 8 * 100:.2f}%\")\n",
    "\n",
    "# extract the inset\n",
    "x_min, y_min, x_max, y_max = rect_coords\n",
    "shp_mask = shapely.Polygon([(x_min, y_min), (x_max, y_min), (x_max, y_max), (x_min, y_max)])\n",
    "mask_rect = ~np.isnan(nc1.rio.clip([shp_mask], drop=False).mask_crt_g.values)\n",
    "\n",
    "# recompute the normalization statistics using the entire image\n",
    "img1 = nc1.band_data.isel(band=[nc.band_data.long_name.index(b) for b in ['R', 'G', 'B']]).values.copy()\n",
    "img1 = img1.transpose(1, 2, 0)\n",
    "\n",
    "# clip and scale the image\n",
    "vmin = np.quantile(img1, 0.01)\n",
    "vmax = np.quantile(img1, 0.99)\n",
    "img1 = np.clip(img1, vmin, vmax)\n",
    "img1 = (img1 - vmin) / (vmax - vmin)\n",
    "\n",
    "nc1_rect = nc1.rio.clip_box(*rect_coords)\n",
    "img1_rect = nc1_rect.band_data.isel(\n",
    "    band=[nc1_rect.band_data.long_name.index(b) for b in ['R', 'G', 'B']]).values.copy()\n",
    "img1_rect = img1_rect.transpose(1, 2, 0)\n",
    "img1_rect = np.clip(img1_rect, vmin, vmax)\n",
    "img1_rect = (img1_rect - vmin) / (vmax - vmin)\n",
    "plt.figure(figsize=(2, 2))\n",
    "plt.imshow(img1_rect)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.tight_layout()\n",
    "plt.savefig(plots_dir / 'inset.png')\n",
    "plt.show()\n"
   ],
   "id": "4c8e6ad082321171",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# decrease the font size to compensate for the smaller figure size\n",
    "original_font_size = matplotlib.rcParams[\"font.size\"]\n",
    "matplotlib.rcParams[\"font.size\"] = \"10\"\n",
    "\n",
    "fig_width_in = 10\n",
    "fig_height_in = 14\n",
    "cbar_args = {'fraction': 0.04725, 'pad': 0.03}\n",
    "\n",
    "plt.figure(figsize=(fig_width_in, fig_height_in), dpi=200)\n",
    "for i in range(1, 25):\n",
    "    plt.subplot(6, 4, i)\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.subplot(6, 4, 1)\n",
    "img = nc1.dhdt.values\n",
    "# vmax_abs = np.nanmax(np.abs(img))\n",
    "vmax_abs = np.nanquantile(np.abs(img), 0.99)\n",
    "plt.imshow(img, cmap='seismic_r', vmin=-vmax_abs, vmax=+vmax_abs)\n",
    "plt.title('elevation change\\n2010-2014')\n",
    "cbar = plt.colorbar(**cbar_args)\n",
    "cbar.set_label('m', labelpad=-10, y=1.1, rotation=0)\n",
    "\n",
    "plt.subplot(6, 4, 2)\n",
    "plt.imshow((g1 - swir1) / (g1 + swir1), cmap='viridis')\n",
    "plt.title(f'NDSI - {y1}')\n",
    "plt.colorbar(**cbar_args)\n",
    "\n",
    "plt.subplot(6, 4, 5)\n",
    "plt.imshow((nir1 - r1) / (nir1 + r1), cmap='viridis')\n",
    "plt.title(f'NDVI - {y1}')\n",
    "plt.colorbar(**cbar_args)\n",
    "\n",
    "plt.subplot(6, 4, 6)\n",
    "plt.imshow((g1 - nir1) / (g1 + nir1), cmap='viridis')\n",
    "plt.title(f'NDWI - {y1}')\n",
    "plt.colorbar(**cbar_args)\n",
    "\n",
    "plt.subplot(6, 4, 10)\n",
    "dem = nc1.dem.values\n",
    "plt.imshow(dem, cmap='terrain')\n",
    "plt.title('DEM')\n",
    "cbar = plt.colorbar(**cbar_args)\n",
    "cbar.set_label('m', labelpad=-10, y=1.1, rotation=0)\n",
    "\n",
    "plt.subplot(6, 4, 11)\n",
    "plt.imshow(np.sin(np.deg2rad(nc1.aspect.values)), cmap='jet')\n",
    "plt.title('sin(aspect)')\n",
    "plt.colorbar(**cbar_args)\n",
    "\n",
    "plt.subplot(6, 4, 12)\n",
    "plt.imshow(np.cos(np.deg2rad(nc1.aspect.values)), cmap='jet')\n",
    "plt.title('cos(aspect)')\n",
    "plt.colorbar(**cbar_args)\n",
    "\n",
    "plt.subplot(6, 4, 13)\n",
    "plt.imshow(nc1.slope.values, cmap='magma')\n",
    "plt.title('slope')\n",
    "cbar = plt.colorbar(**cbar_args)\n",
    "cbar.set_label('deg', labelpad=-5, y=1.1, rotation=0)\n",
    "\n",
    "plt.subplot(6, 4, 14)\n",
    "img = nc1.planform_curvature.values\n",
    "plt.imshow(img, cmap='magma', vmin=np.quantile(img, 0.005), vmax=np.quantile(img, 0.995))\n",
    "plt.title('Planform\\ncurvature')\n",
    "cbar = plt.colorbar(**cbar_args)\n",
    "cbar.set_label(r'm$^{-1}\\cdot 100$', labelpad=-5, y=1.1, rotation=0)\n",
    "\n",
    "plt.subplot(6, 4, 15)\n",
    "img = nc1.profile_curvature.values\n",
    "plt.imshow(img, cmap='magma', vmin=np.quantile(img, 0.005), vmax=np.quantile(img, 0.995))\n",
    "plt.title('Profile\\ncurvature')\n",
    "cbar = plt.colorbar(**cbar_args)\n",
    "cbar.set_label(r'm$^{-1}\\cdot 100$', labelpad=-5, y=1.1, rotation=0)\n",
    "\n",
    "plt.subplot(6, 4, 16)\n",
    "plt.imshow(nc1.terrain_ruggedness_index.values, cmap='magma')\n",
    "plt.title('TRI')\n",
    "cbar = plt.colorbar(**cbar_args)\n",
    "cbar.set_label('m', labelpad=-5, y=1.1, rotation=0)\n",
    "\n",
    "plt.subplot(6, 4, 17)\n",
    "img = nc2.dhdt.values\n",
    "# vmax_abs = np.nanmax(np.abs(img))\n",
    "vmax_abs = np.nanquantile(np.abs(img), 0.99)\n",
    "plt.imshow(img, cmap='seismic_r', vmin=-vmax_abs, vmax=+vmax_abs)\n",
    "plt.title('elevation change\\n2015-2019')\n",
    "cbar = plt.colorbar(**cbar_args)\n",
    "cbar.set_label('m', labelpad=-10, y=1.1, rotation=0)\n",
    "\n",
    "plt.subplot(6, 4, 18)\n",
    "plt.imshow((g2 - swir2) / (g2 + swir2), cmap='viridis')\n",
    "plt.title(f'NDSI - {y2}')\n",
    "plt.colorbar(**cbar_args)\n",
    "\n",
    "plt.subplot(6, 4, 21)\n",
    "plt.imshow((nir2 - r2) / (nir2 + r2), cmap='viridis')\n",
    "plt.title(f'NDVI - {y2}')\n",
    "plt.colorbar(**cbar_args)\n",
    "\n",
    "plt.subplot(6, 4, 22)\n",
    "plt.imshow((g2 - nir2) / (g2 + nir2), cmap='viridis')\n",
    "plt.title(f'NDWI - {y2}')\n",
    "plt.colorbar(**cbar_args)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# reduce the space between the subplots\n",
    "plt.subplots_adjust(hspace=0.15, wspace=0.25)\n",
    "\n",
    "plt.savefig(plots_dir / 'inputs.png', bbox_inches='tight')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# reset the font size\n",
    "matplotlib.rcParams[\"font.size\"] = original_font_size"
   ],
   "id": "1d07ed1fc5e27523",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Figure: scatter plot with predicted areas vs. inventory areas",
   "id": "89fb142ab3781e25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "labels_year = 'inv'\n",
    "# labels_year = '2023'\n",
    "if labels_year == '2023':\n",
    "    fp_results_inv = results_root_dir / 'unet' / 'stats_all_splits' / f'df_stats_calib_agg_s2_alps_plus_manual_2023_{model_version}_ensemble.csv'\n",
    "    df_glacier_agg = pd.read_csv(fp_results_inv)\n",
    "else:\n",
    "    fp_results_inv = results_root_dir / 'unet' / 'stats_all_splits' / f'df_stats_calib_all_s2_alps_plus_inv_{model_version}_ensemble.csv'\n",
    "    df_glacier_agg = pd.read_csv(fp_results_inv)\n",
    "    assert len(df_glacier_agg) == df_glacier_agg.is_inv_year.sum()\n",
    "print(f\"fp_results_inv = {fp_results_inv}\")\n",
    "df_glacier_agg"
   ],
   "id": "e0ef59e9adb14c8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_df = df_glacier_agg.copy()  # only the inventory years\n",
    "y = plot_df.area_inv\n",
    "y_pred = plot_df.area_pred_b20\n",
    "# y_pred = plot_df.area_pred_b0 # only recall\n",
    "\n",
    "plot_df['rel_area_diff'] = (y_pred - y).abs() / y * 100\n",
    "n = len(plot_df)\n",
    "n_better_than_5prc = sum(plot_df.rel_area_diff <= 5)\n",
    "\n",
    "bias = (y_pred - y).mean()\n",
    "mae = sklearn.metrics.mean_absolute_error(y, y_pred)\n",
    "\n",
    "lab = 'inv' if labels_year == 'inv' else 'ref'\n",
    "yl, xl = f'|$A_{{DL4GAM}}$ - $A_{{{lab}}}$| / $A_{{{lab}}}$ * 100 (%)', f'$A_{{{lab}}}$ (km$^2$)'\n",
    "print(f\"total area difference = {y_pred.sum() - y.sum():.2f} km^2 ({100 * (y_pred.sum() - y.sum()) / y.sum():.2f}%)\")\n",
    "label_df = pd.DataFrame({\n",
    "    'x': [11],\n",
    "    'y': [80],\n",
    "    'desc': f\"$A_{{total}}^{{{lab}}}$ = {plot_df.area_inv.sum():.1f} km$^2$\"\n",
    "            f\"\\n$A_{{total}}^{{DL4GAM}}$ = {plot_df.area_pred.sum():.1f} km$^2$\"\n",
    "            f\"\\nn = {n}\"\n",
    "            f\"\\nn$_{{\\\\leq 5\\\\%}}$ = {n_better_than_5prc} ({n_better_than_5prc / n * 100:.1f} %)\"\n",
    "            f\"\\nMAPE = {plot_df.rel_area_diff.mean():.2f} %\"\n",
    "            f\"\\nMAE = {mae:.2f} km$^2$\"\n",
    "})\n",
    "p_size = 2.5\n",
    "\n",
    "# g = (\n",
    "#         p9.ggplot(plot_df, p9.aes(x='area_inv', y='rel_area_diff'))\n",
    "#         + p9.geom_hline(yintercept=5, linetype='--', color=color_list[1], size=1)\n",
    "#         + p9.geom_point(alpha=0.5, stroke=0, size=p_size)\n",
    "#         + p9.theme(figure_size=(7.5, 3.5), dpi=200, text=p9.element_text(size=10))\n",
    "#         + p9.scale_x_log10()\n",
    "#         + p9.labs(title=\"\", x=xl, y=yl)\n",
    "#         + p9.geom_text(p9.aes(label='desc', x='x', y='y'), data=label_df, size=11, ha='left')\n",
    "# )\n",
    "g = (p9.ggplot(plot_df, p9.aes(x='area_inv', y='rel_area_diff')) +\n",
    "     p9.annotate(\n",
    "         \"text\", x=50, y=8, label=\"5% threshold\", color=color_list[1], size=10\n",
    "     ) +\n",
    "     p9.geom_point(alpha=0.5, stroke=0, size=p_size) +\n",
    "     p9.geom_hline(yintercept=5, linetype='--', color=color_list[1], size=1) +\n",
    "     p9.theme(figure_size=(7.5, 3.5), dpi=200, text=p9.element_text(size=10)) +\n",
    "     p9.scale_x_log10() +\n",
    "     p9.ylim([0, 100]) +\n",
    "     p9.labs(title=\"\", x=xl, y=yl) +\n",
    "     p9.geom_text(p9.aes(label='desc', x='x', y='y'), data=label_df, size=11, ha='left'))\n",
    "\n",
    "g.save(plots_dir / f'area_comparison_{labels_year}.pdf')\n",
    "display(g)"
   ],
   "id": "30fcbc9aacd868f5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# sanity check\n",
    "df_changes = pd.read_csv(\n",
    "    '../data/external/_experiments/s2_alps_plus/unet/stats_all_splits/df_changes_stats_calib_all_s2_alps_plus_version_0_ensemble.csv')\n",
    "df_changes = df_changes[df_changes.entry_id.isin(df_t0.entry_id.values)]\n",
    "df_changes = df_changes.sort_values('entry_id')\n",
    "print(df_changes.area_inv.sum(), df_t0.area_inv.sum())\n",
    "print(df_changes.area_t0.sum(), df_t0.area_pred.sum())\n",
    "print(df_changes.area_t1.sum(), df_t1.area_pred.sum())\n",
    "df_changes"
   ],
   "id": "e3140ecbfdf858dd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df_t0 = pd.read_csv(\n",
    "    '../data/external/_experiments/s2_alps_plus/unet/stats_all_splits/df_stats_calib_agg_s2_alps_plus_manual_inv_version_0_ensemble.csv')\n",
    "df_t1 = pd.read_csv(\n",
    "    '../data/external/_experiments/s2_alps_plus/unet/stats_all_splits/df_stats_calib_agg_s2_alps_plus_manual_2023_version_0_ensemble.csv')\n",
    "assert len(df_t0) == len(df_t1)\n",
    "\n",
    "# compute the changes using 1) the 2023 glacier outlines and 2) our predictions\n",
    "area_changes = df_t1.area_inv.values - df_t0.area_inv.values\n",
    "area_changes_pred = df_t1.area_pred.values - df_t0.area_pred.values\n",
    "area_inv_t0 = df_t0.area_inv.values\n",
    "area_pred_t0 = df_t0.area_pred.values\n",
    "plot_df = {\n",
    "    'entry_id': df_t1.entry_id.values,\n",
    "    'change': area_changes,\n",
    "    'change_pred': area_changes_pred,\n",
    "    'change_pred_std': df_changes.area_rate_std.values * 8,\n",
    "    'rel_change': area_changes / area_inv_t0 * 100,\n",
    "    'rel_change_pred': area_changes_pred / area_pred_t0 * 100,\n",
    "    'rel_change_pred_std': df_changes.area_rate_std.values * 8 / area_pred_t0 * 100,\n",
    "    'area_inv': area_inv_t0,\n",
    "    'area_inv_colorbar': area_inv_t0,\n",
    "    'area_inv_pred': df_t0.area_pred.values,\n",
    "    'area_2023': df_t1.area_inv.values,\n",
    "    'area_2023_pred': df_t1.area_pred.values,\n",
    "}\n",
    "plot_df = pd.DataFrame(plot_df)\n",
    "plot_df['is_within_unc'] = (plot_df.change_pred - plot_df.change).abs() <= plot_df.change_pred_std\n",
    "# print(plot_df)\n",
    "\n",
    "yl = f'($A^{{2023}}_{{ref}} - A^{{2015}}_{{inv}}$) / $A^{{2015}}_{{inv}}$ * 100 (%)'\n",
    "xl = f'($A^{{\\\\mu , 2023}}_{{DL4GAM}} - A^{{\\\\mu , 2015}}_{{DL4GAM}}$) / $A^{{\\\\mu , 2015}}_{{DL4GAM}}$ * 100 (%)'\n",
    "\n",
    "corr_s = scipy.stats.spearmanr(plot_df.rel_change_pred, plot_df.rel_change)[0]\n",
    "plot_df_ok = plot_df[plot_df.is_within_unc]\n",
    "corr_s_pl = scipy.stats.spearmanr(plot_df_ok.rel_change_pred, plot_df_ok.rel_change)[0]\n",
    "print(f\"corr_s = {corr_s:.3f}\")\n",
    "sep = ''.join(['-'] * 51)\n",
    "label_df = pd.DataFrame({\n",
    "    'x': [-33],\n",
    "    'y': [-78],\n",
    "    'is_within_unc': [True],\n",
    "    'desc': f\"\\nn = {len(plot_df)}\"\n",
    "            f\"\\n$\\\\rho_{{Spearman}}$ = {corr_s:.3f}\"\n",
    "            f\"\\n{sep}\"\n",
    "            f\"\\n$\\\\Sigma A^{{2015}}_{{inv}}$ = {plot_df.area_inv.sum():.1f} km$^2$\"\n",
    "            f\"\\n$\\\\Sigma A^{{2023}}_{{ref}}$ = {plot_df.area_2023.sum():.1f} km$^2$\"\n",
    "            f\"\\n$\\\\Rightarrow \\\\Delta_{{actual}}$ = {plot_df.area_2023.sum() - plot_df.area_inv.sum():.1f} km$^2$ \"\n",
    "            f\"({(plot_df.area_2023.sum() - plot_df.area_inv.sum()) / plot_df.area_inv.sum() / 8 * 100:.2f} % y$^{{-1}}$)\"\n",
    "            f\"\\n{sep}\"\n",
    "            f\"\\n$\\\\Sigma A^{{2015}}_{{DL4GAM}}$ = {plot_df.area_inv_pred.sum():.1f} km$^2$\"\n",
    "            f\"\\n$\\\\Sigma A^{{2023}}_{{DL4GAM}}$ = {plot_df.area_2023_pred.sum():.1f} km$^2$\"\n",
    "            f\"\\n$\\\\Rightarrow \\\\Delta_{{DL4GAM}}$ = {plot_df.area_2023_pred.sum() - plot_df.area_inv_pred.sum():.1f} km$^2$ \"\n",
    "            f\"({(plot_df.area_2023_pred.sum() - plot_df.area_inv_pred.sum()) / plot_df.area_inv_pred.sum() / 8 * 100:.2f} % y$^{{-1}}$)\"\n",
    "            f\"\\n{sep}\"\n",
    "            f\"\\n$1 \\\\sigma$ coverage: {len(plot_df_ok) / len(plot_df) * 100:.1f} % \"\n",
    "            f\"\\n(nominal = 68.2%)\"\n",
    "})\n",
    "p_size = 2.5\n",
    "max_prc_diff = 10\n",
    "display(plot_df[plot_df.rel_change_pred > max_prc_diff])\n",
    "g = (\n",
    "        p9.ggplot(plot_df, p9.aes(x='rel_change_pred', y='rel_change', color='area_inv', shape='is_within_unc')) +\n",
    "        # p9.geom_point(alpha=0.7, stroke=0, size=p_size) +\n",
    "        p9.geom_abline(slope=1, intercept=0, linetype='dashed', color='gray', size=0.8) +\n",
    "        p9.geom_point(alpha=1.0, size=p_size, stroke=1) +\n",
    "        p9.scale_shape_manual(values=['x', 'o'], name='Within $1\\\\sigma$',\n",
    "                              labels=['No', 'Yes']) +\n",
    "        p9.scale_color_cmap('viridis', name='$A_{{inv}}$ (km²)', trans='log10',\n",
    "                            limits=(plot_df.area_inv.min(), 20)) +\n",
    "        p9.theme(\n",
    "            figure_size=(7, 6.5), dpi=200,\n",
    "            text=p9.element_text(size=9),\n",
    "            legend_key_width=10,\n",
    "            legend_key_height=100,\n",
    "        ) +\n",
    "        p9.xlim([-100, max_prc_diff]) +\n",
    "        p9.ylim([-100, max_prc_diff]) +\n",
    "        p9.labs(title=\"\", x=xl, y=yl) +\n",
    "        p9.geom_text(p9.aes(label='desc', x='x', y='y'), data=label_df, size=8.5, ha='left', color=\"black\") +\n",
    "        p9.theme(legend_position='right') +\n",
    "        p9.coord_fixed(ratio=1)\n",
    ")\n",
    "g.save(plots_dir / 'change_rates_scatter.pdf')\n",
    "display(g)"
   ],
   "id": "2dbd50d06ebf8f07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Figure: scatter plot with predicted uncertainties vs. 1) errors and 2) debris areas (CH only)",
   "id": "3aa8f38a9aa2964a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_df = df_glacier_agg.copy()  # only the inventory years\n",
    "\n",
    "# add the country\n",
    "plot_df = plot_df.merge(gl_df[['entry_id', 'Country']])\n",
    "\n",
    "y = plot_df.area_inv\n",
    "y_pred = plot_df.area_pred_b20\n",
    "# y_pred = plot_df.area_pred_b0 # only recall\n",
    "plot_df['rel_area_diff'] = (y_pred - y).abs() / y * 100\n",
    "plot_df['area_pred_std'] = (plot_df.area_pred_high - plot_df.area_pred_low) / 2\n",
    "plot_df['rel_area_unc'] = plot_df['area_pred_std'] / np.maximum(y_pred, 1e-12) * 100\n",
    "\n",
    "q = 0.98\n",
    "# drop the outliers (cause by very low predicted areas)\n",
    "max_rel_area_unc = plot_df.rel_area_unc.quantile(q)\n",
    "print(f\"max_rel_area_unc = {max_rel_area_unc}\")\n",
    "plot_sdf = plot_df[plot_df.rel_area_unc <= max_rel_area_unc]\n",
    "print(len(plot_sdf), len(plot_sdf) - len(plot_df))\n",
    "\n",
    "yl = f'|$A^{{\\\\mu}}_{{DL4GAM}} - A_{{inv}}$| / $A_{{inv}}$ * 100 (%)'\n",
    "xl = f'$A^{{\\\\sigma}}_{{DL4GAM}} / A^{{\\\\mu}}_{{DL4GAM}}$ * 100 (%)'\n",
    "\n",
    "corr_s = scipy.stats.spearmanr(plot_sdf.rel_area_unc, plot_sdf.rel_area_diff)[0]\n",
    "print(f\"corr_s = {corr_s:.3f}\")\n",
    "label_df = pd.DataFrame({\n",
    "    'x': [60],\n",
    "    'y': [95],\n",
    "    'desc': f\"n = {len(plot_sdf)}\"\n",
    "            f\"\\n$\\\\rho_{{Spearman}}$ = {corr_s:.3f}\"\n",
    "})\n",
    "p_size = 2.5\n",
    "g = (\n",
    "        p9.ggplot(plot_sdf, p9.aes(x='rel_area_unc', y='rel_area_diff'))\n",
    "        + p9.geom_point(alpha=0.5, stroke=0, size=p_size)\n",
    "        + p9.theme(figure_size=(4, 4), dpi=200, text=p9.element_text(size=10))\n",
    "        + p9.xlim([0, 100])\n",
    "        + p9.ylim([0, 100])\n",
    "        + p9.labs(title=\"\", x=xl, y=yl)\n",
    "        + p9.geom_text(p9.aes(label='desc', x='x', y='y'), data=label_df, size=11, ha='left')\n",
    ")\n",
    "g.save(plots_dir / 'uncertainties_vs_errors.pdf')\n",
    "display(g)"
   ],
   "id": "ed0860307a11e1bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# choose only the glaciers from CH & with a debris cover > 1%\n",
    "plot_df['debris_prc'] = plot_df.area_debris / plot_df.area_inv * 100\n",
    "plot_sdf = plot_df[(plot_df.Country == 'CH') & (plot_df.debris_prc >= 1)]\n",
    "\n",
    "q = 0.98\n",
    "# drop the outliers (cause by very low predicted areas)\n",
    "n_init = len(plot_sdf)\n",
    "max_rel_area_unc = plot_df.rel_area_unc.quantile(q)\n",
    "print(f\"max_rel_area_unc = {max_rel_area_unc}\")\n",
    "plot_sdf = plot_sdf[plot_df.rel_area_unc <= max_rel_area_unc]\n",
    "print(len(plot_sdf), len(plot_sdf) - n_init)\n",
    "\n",
    "# yl, xl = f'|DL4GAM predicted area - inventory area| (km$^2$)', 'DL4GAM area uncertainty (km$^2$)'\n",
    "\n",
    "yl = f'Debris coverage (%)'\n",
    "xl = f'$A^{{\\\\sigma}}_{{DL4GAM}} / A^{{\\\\mu}}_{{DL4GAM}}$ * 100 (%)'\n",
    "\n",
    "corr_s = scipy.stats.spearmanr(plot_sdf.rel_area_unc, plot_sdf.debris_prc)[0]\n",
    "print(f\"corr_s = {corr_s:.3f}\")\n",
    "label_df = pd.DataFrame({\n",
    "    'x': [60],\n",
    "    'y': [95],\n",
    "    'desc': f\"n = {len(plot_sdf)}\"\n",
    "            f\"\\n$\\\\rho_{{Spearman}}$ = {corr_s:.3f}\"\n",
    "})\n",
    "p_size = 2.5\n",
    "g = (\n",
    "        p9.ggplot(plot_sdf, p9.aes(x='rel_area_unc', y='debris_prc'))\n",
    "        + p9.geom_point(alpha=0.5, stroke=0, size=p_size)\n",
    "        + p9.theme(figure_size=(4, 4), dpi=200, text=p9.element_text(size=10))\n",
    "        + p9.xlim([0, 100])\n",
    "        + p9.labs(title=\"\", x=xl, y=yl)\n",
    "        + p9.geom_text(p9.aes(label='desc', x='x', y='y'), data=label_df, size=11, ha='left')\n",
    ")\n",
    "g.save(plots_dir / 'uncertainties_vs_debris.pdf')\n",
    "display(g)"
   ],
   "id": "982d062c79ed16c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Figure: SGI2016 comparison\n",
    "\n",
    "The data is taken from Table S6 (https://www.frontiersin.org/journals/earth-science/articles/10.3389/feart.2021.704189/full#supplementary-material).\n",
    "1) scatter plot with predicted areas vs. average area across experts\n",
    "2) scatter plot with predicted uncertainties vs. area standard deviation across experts"
   ],
   "id": "f104c3adefe38682"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fp = model_root_dir / 'stats_all_splits' / f'df_stats_calib_agg_s2_sgi_inv_{model_version}_ensemble.csv'\n",
    "print(fp)\n",
    "df_glacier_agg = pd.read_csv(fp)\n",
    "df_glacier_agg"
   ],
   "id": "1c719a16ffbe653d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "s6_df = pd.read_csv('../data/outlines/sgi2016/table_s6.csv')\n",
    "s6_df = s6_df[s6_df.entry_id.isin(df_glacier_agg.entry_id)]\n",
    "print('|'.join([f\"{x}\" for x in s6_df.entry_id]))\n",
    "display(s6_df)\n",
    "\n",
    "tdf = df_glacier_agg[df_glacier_agg.entry_id.isin(s6_df.entry_id)].sort_values('area_inv')[\n",
    "    ['entry_id', 'area_inv', 'area_pred', 'area_pred_std', 'area_debris', 'area_debris_pred_b20']].merge(\n",
    "    s6_df.drop(columns=['Glacier name', 'stddev_area_prc']), on='entry_id')\n",
    "display(tdf)"
   ],
   "id": "95890740aa4b5475",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_df = pd.DataFrame({\n",
    "    'area_inv': tdf.area_inv,\n",
    "    'area_inv_std': tdf.stddev_area,\n",
    "    'area_pred': tdf.area_pred,\n",
    "    'area_pred_std': tdf.area_pred_std,\n",
    "})\n",
    "\n",
    "y = plot_df.area_inv\n",
    "y_pred = plot_df.area_pred\n",
    "# y_pred = plot_df.area_pred_b0 # only recall\n",
    "\n",
    "plot_df['rel_area_diff'] = (y_pred - y).abs() / y * 100\n",
    "n = len(plot_df)\n",
    "n_better_than_5prc = sum(plot_df.rel_area_diff <= 5)\n",
    "\n",
    "bias = (y_pred - y).mean()\n",
    "mae = sklearn.metrics.mean_absolute_error(y, y_pred)\n",
    "\n",
    "lab = 'SGI'\n",
    "yl, xl = f'|$A_{{DL4GAM}}$ - $A_{{{lab}}}$| / $A_{{{lab}}}$ * 100 (%)', f'$A_{{{lab}}}$ (km$^2$)'\n",
    "print(f\"total area difference = {y_pred.sum() - y.sum():.2f} km^2 ({100 * (y_pred.sum() - y.sum()) / y.sum():.2f}%)\")\n",
    "label_df = pd.DataFrame({\n",
    "    'x': [5],\n",
    "    'y': [50],\n",
    "    'desc': f\"$A_{{total}}^{{{lab}}}$ = {plot_df.area_inv.sum():.1f} km$^2$\"\n",
    "            f\"\\n$A_{{total}}^{{DL4GAM}}$ = {plot_df.area_pred.sum():.1f} km$^2$\"\n",
    "            f\"\\nn = {n}\"\n",
    "            f\"\\nn$_{{\\\\leq 5\\\\%}}$ = {n_better_than_5prc} ({n_better_than_5prc / n * 100:.1f} %)\"\n",
    "            f\"\\nMAPE = {plot_df.rel_area_diff.mean():.2f} %\"\n",
    "            f\"\\nMAE = {mae:.2f} km$^2$\"\n",
    "})\n",
    "p_size = 3.5\n",
    "\n",
    "# g = (\n",
    "#         p9.ggplot(plot_df, p9.aes(x='area_inv', y='rel_area_diff'))\n",
    "#         + p9.geom_hline(yintercept=5, linetype='--', color=color_list[1], size=1)\n",
    "#         + p9.geom_point(alpha=0.5, stroke=0, size=p_size)\n",
    "#         + p9.theme(figure_size=(7.5, 3.5), dpi=200, text=p9.element_text(size=10))\n",
    "#         + p9.scale_x_log10()\n",
    "#         + p9.labs(title=\"\", x=xl, y=yl)\n",
    "#         + p9.geom_text(p9.aes(label='desc', x='x', y='y'), data=label_df, size=11, ha='left')\n",
    "# )\n",
    "g = (p9.ggplot(plot_df, p9.aes(x='area_inv', y='rel_area_diff')) +\n",
    "     p9.annotate(\n",
    "         \"text\", x=15, y=8, label=\"5% threshold\", color=color_list[1], size=10\n",
    "     ) +\n",
    "     p9.geom_point(alpha=0.9, stroke=0, size=p_size) +\n",
    "     p9.geom_hline(yintercept=5, linetype='--', color=color_list[1], size=1) +\n",
    "     p9.theme(figure_size=(5.5, 3.5), dpi=200, text=p9.element_text(size=10)) +\n",
    "     p9.scale_x_log10() +\n",
    "     p9.ylim([0, 100]) +\n",
    "     p9.labs(title=\"\", x=xl, y=yl) +\n",
    "     p9.geom_text(p9.aes(label='desc', x='x', y='y'), data=label_df, size=11, ha='left'))\n",
    "\n",
    "g.save(plots_dir / f'sgi_area_comparison.pdf')\n",
    "display(g)\n",
    "\n",
    "plot_df['rel_area_inv_std'] = plot_df['area_inv_std'] / plot_df['area_inv'] * 100\n",
    "plot_df['rel_area_unc'] = plot_df['area_pred_std'] / y_pred * 100\n",
    "\n",
    "yl = f'Relative uncertainty across SGI2016 experts (%)'\n",
    "xl = f'$A^{{\\\\sigma}}_{{DL4GAM}} / A^{{\\\\mu}}_{{DL4GAM}}$ * 100 (%)'\n",
    "\n",
    "corr_s = scipy.stats.spearmanr(plot_df.rel_area_unc, plot_df.rel_area_inv_std)[0]\n",
    "print(f\"corr_s = {corr_s:.3f}\")\n",
    "label_df = pd.DataFrame({\n",
    "    'x': [60],\n",
    "    'y': [95],\n",
    "    'desc': f\"n = {len(plot_df)}\"\n",
    "            f\"\\n$\\\\rho_{{Spearman}}$ = {corr_s:.3f}\"\n",
    "})\n",
    "g = (\n",
    "        p9.ggplot(plot_df, p9.aes(x='rel_area_unc', y='rel_area_inv_std'))\n",
    "        + p9.geom_point(alpha=0.9, stroke=0, size=p_size)\n",
    "        + p9.theme(figure_size=(4, 4), dpi=200, text=p9.element_text(size=10))\n",
    "        + p9.xlim([0, 25])\n",
    "        + p9.ylim([0, 25])\n",
    "        + p9.labs(title=\"\", x=xl, y=yl)\n",
    "        + p9.geom_abline(linetype='--', color='gray')\n",
    "    # + p9.geom_text(p9.aes(label='desc', x='x', y='y'), data=label_df, size=11, ha='left')\n",
    ")\n",
    "g.save(plots_dir / 'sgi_stddev_comparison.pdf')\n",
    "display(g)"
   ],
   "id": "4d6c0d6d2e8e1809",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Figure & table for the rates interpolation part (for the appendix)",
   "id": "195c330b35d833f3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Figure with the area change rates for each glacier and the piece-wise linear model",
   "id": "14bfe4ad9bf759ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fp = (\n",
    "        model_root_dir / 'stats_all_splits' /\n",
    "        f\"df_changes_stats_calib_all_s2_alps_plus_{model_version}_ensemble_extrapolated.csv\"\n",
    ")\n",
    "print(f\"Reading the glacier area change rates dataframe from {fp}\")\n",
    "df_rates_all_g = pd.read_csv(fp)\n",
    "df_rates_all_g.area_class = df_rates_all_g.area_class.apply(lambda x: x.replace('>=', r'$\\geq$'))\n",
    "area_classes = df_rates_all_g.area_class.unique()\n",
    "df_rates_all_g['area_class'] = pd.Categorical(df_rates_all_g.area_class, categories=area_classes, ordered=True)\n",
    "\n",
    "# extract only those glaciers for which direct estimates are available (i.e. not interpolated)\n",
    "df_rates = df_rates_all_g[~df_rates_all_g.area_rate_orig.isna()]\n",
    "\n",
    "# sdf = df_rates_all_g[~df_rates_all_g.filtered.isna()]\n",
    "# sdf[sdf.entry_id.isin([\"g_1292\", \"g_0819\", \"g_0828\", \"g_2662\", \"g_3644\", \"g_3643\"])]\n",
    "\n",
    "df_rates_all_g"
   ],
   "id": "f5c954c0be555e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = df_rates_all_g[df_rates_all_g.filtered == False]\n",
    "\n",
    "plt.figure(dpi=200, figsize=(10, 4))\n",
    "plt.scatter(df.area_inv, df.area_rate_prc * 100, label='DL4GAM estimates', color=color_list[0])\n",
    "plt.gca().set_xscale('log')\n",
    "df_rates_all_g = df_rates_all_g.sort_values('area_inv')\n",
    "plt.plot(df_rates_all_g.area_inv, df_rates_all_g.area_rate_prc_pred * 100, color=color_list[1], linestyle='--',\n",
    "         linewidth=2.5, label='Quadratic fit')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.xlabel('Glacier inventory area (km$^2$)')\n",
    "plt.ylabel(r'Relative area change (% y$^{-1}$)')\n",
    "\n",
    "# equation_text = rf\"$\\Delta$A/A = {a:.3f} {'+' if b >= 0 else '-'} {abs(b):.3f} $\\cdot$ log$_{{10}}$(A) {'+' if c >= 0 else '-'} {abs(c):.3f} $\\cdot$ log$_{{10}}$(A)^2\"\n",
    "# plt.text(0.545, 0.07, equation_text, fontsize=10, transform=plt.gca().transAxes,\n",
    "#          bbox=dict(facecolor='white', edgecolor='gray', alpha=0.8))\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.savefig(plots_dir / 'area_change_rates_extrapolation.pdf', bbox_inches='tight')\n",
    "\n",
    "plt.show()\n"
   ],
   "id": "e4b34eabcad49050",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "table_df = {'Step': [], 'Count': [], 'area': []}\n",
    "\n",
    "# add the initial stats\n",
    "t_n = len(gl_df)\n",
    "t_a = gl_df.area_inv.sum()\n",
    "table_df['Step'].append('Inventory')\n",
    "table_df['Count'].append(f\"{t_n} (100%)\")\n",
    "table_df['area'].append(f\"{t_a:.1f} km$^2$ (100%)\")\n",
    "\n",
    "# add the stats of the covered glaciers\n",
    "table_df['Step'].append('Our dataset')\n",
    "sdf = df_rates_all_g[~df_rates_all_g.filtered.isna()]\n",
    "table_df['Count'].append(f\"{len(sdf)} ({len(sdf) / t_n * 100:.1f}%)\")\n",
    "table_df['area'].append(f\"{sdf.area_inv.sum():.1f} km$^2$ ({sdf.area_inv.sum() / t_a * 100:.1f}%)\")\n",
    "\n",
    "# filtering using the uncertainties\n",
    "sdf = df_rates_all_g[df_rates_all_g.filtered_by_unc == False]\n",
    "table_df['Step'].append(r'After uncertainty-based filtering (SNR = $\\frac{\\mu}{\\sigma}\\leq$ 1)')\n",
    "table_df['Count'].append(f\"{len(sdf)} ({len(sdf) / t_n * 100:.1f}%)\")\n",
    "table_df['area'].append(f\"{sdf.area_inv.sum():.1f} km$^2$ ({sdf.area_inv.sum() / t_a * 100:.1f}%)\")\n",
    "\n",
    "# filtering using the uncertainties and recall\n",
    "sdf = df_rates_all_g[df_rates_all_g.filtered == False]\n",
    "table_df['Step'].append(r'After filtering by recall ($\\geq 90%$)')\n",
    "table_df['Count'].append(f\"{len(sdf)} ({len(sdf) / t_n * 100:.1f}%)\")\n",
    "table_df['area'].append(f\"{sdf.area_inv.sum():.1f} km$^2$ ({sdf.area_inv.sum() / t_a * 100:.1f}%)\")\n",
    "\n",
    "table_df = pd.DataFrame(table_df)\n",
    "display(table_df)\n",
    "\n",
    "out = table_df.style.hide(axis=\"index\").to_latex(\n",
    "    hrules=True,\n",
    "    convert_css=True,\n",
    "    column_format=''.join(['c'] * len(table_df.columns))\n",
    ")\n",
    "\n",
    "out = out.replace('%', '\\\\%')\n",
    "for x in out.splitlines():\n",
    "    print(f\"\\t{x}\")\n",
    "\n",
    "# out = out.replace('_', '\\\\_')\n",
    "# out = out.replace('font\\\\-weightbold', 'font-weightbold')\n",
    "# print(out)"
   ],
   "id": "490ed8108790c632",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = df_rates_all_g.copy()\n",
    "\n",
    "extr_stats = {'n_covered': [], 'n_missing': [], 'area_covered': [], 'area_missing': []}\n",
    "for i, crt_area_class in enumerate(area_classes):\n",
    "    idx = (df.area_class == crt_area_class)\n",
    "    x = df.area_inv[idx].mean()\n",
    "    idx_ok = idx & (df.filtered == False)\n",
    "    n = sum(idx)\n",
    "    n_ok = sum(idx_ok)\n",
    "    ta = df[idx].area_inv.sum()\n",
    "    ta_ok = df[idx_ok].area_inv.sum()\n",
    "    extr_stats['n_covered'].append(n_ok)\n",
    "    extr_stats['n_missing'].append(n - n_ok)\n",
    "    extr_stats['area_covered'].append(ta_ok)\n",
    "    extr_stats['area_missing'].append(ta - ta_ok)\n",
    "extr_stats_df = pd.DataFrame(extr_stats)\n",
    "extr_stats_df.insert(0, 'area_class', area_classes)\n",
    "extr_stats_df.insert(3, 'n', extr_stats_df.n_covered + extr_stats_df.n_missing)\n",
    "extr_stats_df.insert(len(extr_stats_df.columns), 'area', extr_stats_df.area_covered + extr_stats_df.area_missing)\n",
    "\n",
    "# add a line with the totals\n",
    "extr_stats_df.loc[len(extr_stats_df)] = ['all'] + extr_stats_df.sum().tolist()[1:]\n",
    "extr_stats_df.columns = [\n",
    "    'Size class',\n",
    "    r'$n_{covered}$',\n",
    "    r'$n_{missing}$',\n",
    "    '$n$',\n",
    "    r'$A_{covered} (km^2)$',\n",
    "    r'$A_{missing} (km^2)$',\n",
    "    '$A (km^2)$'\n",
    "]\n",
    "extr_stats_df['Size class'] = extr_stats_df['Size class'].apply(lambda s: f\"${s}$\")\n",
    "display(extr_stats_df)\n",
    "\n",
    "format_dict = {\n",
    "    c: v for c, v in\n",
    "    zip(extr_stats_df.columns, ['{}', '{:d}', '{:d}', '{:d}', '{:.2f}', '{:.2f}', '{:.2f}'])\n",
    "}\n",
    "out = extr_stats_df.style.format(format_dict).hide(axis=\"index\").to_latex(\n",
    "    hrules=True,\n",
    "    convert_css=True,\n",
    "    column_format=''.join(['c'] * len(extr_stats_df.columns))\n",
    ")\n",
    "\n",
    "out = out.replace(r'$\\geq$', r'\\geq')\n",
    "\n",
    "for x in out.splitlines():\n",
    "    if 'all' in x:\n",
    "        print('\\t\\\\midrule')\n",
    "    print(f\"\\t{x}\")"
   ],
   "id": "7b2e7003da42a6a7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Figures with the glacier extent & with the gridded area change rates, with the hillshade in the background",
   "id": "2ffacbcb4b12e850"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "gl_df_with_rates_ok = gl_df_with_rates[gl_df_with_rates.filtered == False].copy()\n",
    "gl_df_with_rates_ok"
   ],
   "id": "e7ecb34da2c050ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "matplotlib.rcParams[\"font.family\"] = \"serif\"  # cmr doesn't work here",
   "id": "815d07ddfa98b9c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot only the glaciers covered in our dataset\n",
    "gl_df_with_rates = gl_df.merge(df_rates, on='entry_id', suffixes=('', '_y'))"
   ],
   "id": "26ca2339501fafc7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "crs = ccrs.EuroPP()\n",
    "dem = xr.open_dataarray('../data/external/copdem_90m/copdem_90m_alps.tif')\n",
    "dem = dem.isel(band=0)\n",
    "dem = dem.rio.reproject(crs, resolution=500, resampling=rio.enums.Resampling.bilinear)\n",
    "\n",
    "# set the bounds and convert to UTM\n",
    "min_y, max_y, min_x, max_x = [44.5, 47.75, 5.75, 13.75]\n",
    "min_x, min_y = crs.transform_point(min_x, min_y, ccrs.PlateCarree())\n",
    "max_x, max_y = crs.transform_point(max_x, max_y, ccrs.PlateCarree())\n",
    "min_x, min_y = int(min_x), int(min_y)\n",
    "max_x, max_y = int(max_x), int(max_y)\n",
    "\n",
    "# clip the DEM and compute the hillshade\n",
    "dem = dem.rio.clip_box(min_x, min_y, max_x, max_y, crs=crs)\n",
    "hillshade = es.hillshade(dem.values, azimuth=180, altitude=75)\n",
    "hillshade = gaussian_filter(hillshade, sigma=0.5)\n",
    "\n",
    "# make sure the bounds are not too small\n",
    "gl_df_with_rates_proj = gl_df_with_rates.to_crs(crs)\n",
    "assert min_x < gl_df_with_rates_proj.bounds.min().minx\n",
    "assert max_x > gl_df_with_rates_proj.bounds.max().maxx\n",
    "assert min_y < gl_df_with_rates_proj.bounds.min().miny\n",
    "assert max_y > gl_df_with_rates_proj.bounds.max().maxy\n",
    "fig_width = 10\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    nrows=1,\n",
    "    dpi=250,\n",
    "    figsize=(fig_width, fig_width * (max_y - min_y) / (max_x - min_x)),\n",
    "    subplot_kw={'projection': crs}\n",
    ")\n",
    "\n",
    "ext = [min_x, max_x, min_y, max_y]\n",
    "\n",
    "# draw the hillshade (after darkening it)\n",
    "hillshade = (hillshade - hillshade.min()) / (hillshade.max() - hillshade.min())\n",
    "# hillshade = (hillshade / hillshade.max()) ** 3\n",
    "# hillshade = hillshade * 0.5 + 0.5\n",
    "hillshade = hillshade * 0.3\n",
    "ax.imshow(hillshade, extent=ext, cmap='gray', zorder=-1, interpolation='nearest', alpha=0.9, vmin=0, vmax=1)\n",
    "\n",
    "# draw the glacier outlines\n",
    "# gl_df_with_rates_proj.simplify(100).plot(ax=ax, edgecolor='none', facecolor='#1434A4')\n",
    "# gl_df_with_rates_proj.simplify(100).plot(ax=ax, edgecolor='none', facecolor='#0818A8', zorder=1)\n",
    "gl_df_with_rates_proj.simplify(100).plot(ax=ax, edgecolor='none', facecolor='#87CEEB', zorder=1)\n",
    "\n",
    "# add axis ticks\n",
    "gl = ax.gridlines(draw_labels=True, linestyle='--', alpha=0.5, zorder=0, color='gray')\n",
    "gl.zorder = 0\n",
    "\n",
    "# add the country borders and the country names\n",
    "ax.add_feature(cartopy.feature.BORDERS, zorder=0, linestyle='-', linewidth=0.75, alpha=1.0, color='orange')\n",
    "country_coords = {\n",
    "    'France': (6.25, 45.5),\n",
    "    'Switzerland': (7.0, 46.5),\n",
    "    'Austria': (11.5, 47.15),\n",
    "    'Italy': (11.5, 46.5)\n",
    "}\n",
    "for cn, (x, y) in country_coords.items():\n",
    "    ax.annotate(\n",
    "        cn,\n",
    "        xy=crs.transform_point(x, y, ccrs.PlateCarree()),\n",
    "        xytext=(5, 5),\n",
    "        textcoords='offset points',\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=10,\n",
    "        color='black',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', fc='white', ec='gray', lw=0.5)\n",
    "    )\n",
    "\n",
    "# add the scalebar\n",
    "ax.add_artist(ScaleBar(dx=1.0, length_fraction=0.25, font_properties={'size': 10}, location='lower right'))\n",
    "plt.tight_layout()\n",
    "\n",
    "# export to pdf\n",
    "plt.savefig(plots_dir / 'spatial_extent_glaciers.pdf', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ],
   "id": "325e70fa818fc63c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# group the glaciers into cells of 10km x 10km cells using the centroid\n",
    "dx = 10e3\n",
    "\n",
    "# use only the rates which were not filtered\n",
    "gl_df_with_rates_ok = gl_df_with_rates[gl_df_with_rates.filtered == False].copy()\n",
    "\n",
    "plot_0205 = False\n",
    "if plot_0205:\n",
    "    gl_df_with_rates_ok = gl_df_with_rates_ok[gl_df_with_rates_ok.area_class == '[0.2, 0.5)']\n",
    "\n",
    "t = pyproj.Transformer.from_crs(gl_df_with_rates_ok.crs, crs, always_xy=True)\n",
    "centroids_wgs84 = t.transform(gl_df_with_rates_ok.centroid.x, gl_df_with_rates_ok.centroid.y)\n",
    "gl_df_with_rates_ok['lat'] = centroids_wgs84[0]\n",
    "gl_df_with_rates_ok['lon'] = centroids_wgs84[1]\n",
    "\n",
    "gl_df_with_rates_ok['lat_bin'] = (gl_df_with_rates_ok.lat // dx) * dx\n",
    "gl_df_with_rates_ok['lon_bin'] = (gl_df_with_rates_ok.lon // dx) * dx\n",
    "gdf_avg_s = gl_df_with_rates_ok.groupby(['lat_bin', 'lon_bin']).agg(\n",
    "    {'area_rate': 'sum', 'entry_id': 'count', 'area_inv': 'sum'}).reset_index()\n",
    "gdf_avg_s['area_rate_prc'] = gdf_avg_s.area_rate / gdf_avg_s.area_inv * 100\n",
    "gdf_avg_s"
   ],
   "id": "2594c9b1372fbe58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, axes = plt.subplots(\n",
    "    nrows=2,\n",
    "    dpi=250,\n",
    "    figsize=(fig_width, fig_width * (max_y - min_y) / (max_x - min_x) * 1.03 + 0.1),\n",
    "    gridspec_kw={\"height_ratios\": [1, 0.05], 'hspace': 0.15},\n",
    "    subplot_kw={'projection': crs}\n",
    ")\n",
    "\n",
    "### 1) draw the glacier outlines\n",
    "ax = axes[0]\n",
    "\n",
    "# Apply gamma correction and rescale brightness\n",
    "gamma = 10  # slightly brighten the mid-tones (lower is brighter)\n",
    "hillshade_g = hillshade ** (1 / gamma)\n",
    "\n",
    "# draw the hillshade (processed in the previous cell)\n",
    "ax.imshow(hillshade_g, extent=ext, cmap='gray', zorder=-1, interpolation='nearest', alpha=0.9, vmin=0, vmax=1)\n",
    "\n",
    "# add axis ticks\n",
    "gl = ax.gridlines(draw_labels=True, linestyle='--', alpha=0.5, zorder=0, color='gray')\n",
    "gl.zorder = 0\n",
    "\n",
    "# keep only the cells with more than 1 km2 glacierized areas, to enhance the contrast\n",
    "if plot_0205:\n",
    "    tdf = gdf_avg_s[gdf_avg_s.area_inv >= 0.5].copy()\n",
    "    area_factor = 30\n",
    "    custom_sizes = [0.5, 1, 2]\n",
    "else:\n",
    "    area_factor = 2\n",
    "    custom_sizes = [5, 25, 50, 100]\n",
    "    tdf = gdf_avg_s[gdf_avg_s.area_inv >= 2].copy()\n",
    "\n",
    "sc = ax.scatter(tdf.lat_bin, tdf.lon_bin, s=tdf.area_inv * area_factor, c=tdf.area_rate_prc, cmap='inferno', zorder=2)\n",
    "fig.colorbar(\n",
    "    sc, ax=axes[1], label='Annual area change rate (% $\\\\cdot y^{-1}$)', orientation=\"horizontal\", fraction=0.9,\n",
    "    pad=0.01\n",
    ")\n",
    "\n",
    "# define custom sizes for the legend\n",
    "legend_elements = [\n",
    "    matplotlib.lines.Line2D(\n",
    "        [0], [0],\n",
    "        marker='o',\n",
    "        color='w',\n",
    "        markerfacecolor='gray',\n",
    "        markersize=np.sqrt(size * area_factor),\n",
    "        label=f'{size}',\n",
    "        linestyle='None'\n",
    "    )\n",
    "    for size in custom_sizes\n",
    "]\n",
    "ax.legend(handles=legend_elements, title=\"initial\\narea\\n($km^2$)\", loc='lower right')\n",
    "\n",
    "# add the country borders and the country names\n",
    "ax.add_feature(cartopy.feature.BORDERS, zorder=0, linestyle='-', linewidth=0.5, alpha=0.75, color='orange')\n",
    "\n",
    "# add the scalebar\n",
    "ax.add_artist(ScaleBar(dx=1.0, length_fraction=0.25, font_properties={'size': 10}, location='upper left'))\n",
    "plt.tight_layout()\n",
    "\n",
    "# export to pdf\n",
    "fn = 'area_change_rates_map_0205' if plot_0205 else 'area_change_rates_map'\n",
    "plt.savefig(plots_dir / f'{fn}.pdf', bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ],
   "id": "1e473c1c90dedf30",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_df = gl_df_with_rates_ok.copy()\n",
    "plot_df.area_rate_prc = plot_df.area_rate_prc * 8  # use cumulative changes\n",
    "plot_df['area_class'] = plot_df['area_class'].apply(lambda s: f\"${s}$\".replace(r'$\\geq$', r'\\geq'))\n",
    "\n",
    "plot_df_text = plot_df.groupby('area_class', observed=True).agg(\n",
    "    {'year_t0': 'count', 'area_inv': 'sum', 'area_rate_prc': 'min'}).reset_index()\n",
    "plot_df_text['area_total_prc'] = plot_df_text.area_inv / df_rates.area_inv.sum() * 100\n",
    "plot_df_text['y_pos'] = plot_df_text.area_rate_prc * 100 * 0.75\n",
    "plot_df_text['desc_n'] = plot_df_text.apply(lambda r: f'n = {r.year_t0}', axis=1)\n",
    "plot_df_text['desc_area'] = plot_df_text.apply(lambda r: rf'A_{{\\%}} = {r.area_total_prc:.1f}\\%', axis=1)\n",
    "\n",
    "plot_df_text"
   ],
   "id": "931df9248c4685b0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "y_min = round(plot_df.area_rate_prc.min() * 100 * 1.2)\n",
    "y_max = round(plot_df.area_rate_prc.max() * 100)\n",
    "g = (\n",
    "        p9.ggplot(plot_df, p9.aes(x='area_class', y='area_rate_prc * 100'))\n",
    "        + p9.geom_boxplot()\n",
    "        + p9.geom_text(plot_df_text, p9.aes(x='area_class', label='desc_n'), y=y_min * 0.875, va='top', size=10,\n",
    "                       parse=True)\n",
    "        + p9.geom_text(plot_df_text, p9.aes(x='area_class', label='desc_area'), y=y_min * 0.95, va='top', size=10,\n",
    "                       parse=True)\n",
    "        + p9.labs(\n",
    "    y=r'2023-2015 relative area change (%)',\n",
    "    x=r'Glacier size class (km$^2$)'\n",
    ")\n",
    "        + p9.ylim(y_min, y_max)\n",
    "        + p9.theme(\n",
    "    figure_size=(8, 4),\n",
    "    dpi=200,\n",
    "    legend_direction='horizontal',\n",
    "    legend_position='bottom',\n",
    "    text=p9.element_text(family='cmr10', size=12)\n",
    ")\n",
    ")\n",
    "display(g)\n",
    "\n",
    "fp = plots_dir / 'area_change_per_class.pdf'\n",
    "fp.parent.mkdir(parents=True, exist_ok=True)\n",
    "g.save(fp)\n"
   ],
   "id": "4aa0111999b90e03",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plot_df = gl_df_with_rates_ok.rename(\n",
    "    columns={'ELEV_MED': 'elev_med', 'ASP_MEAN': 'aspect', 'SLOPE_MEAN': 'slope'}).copy()\n",
    "plot_df.area_rate_prc = plot_df.area_rate_prc * 8  # use cumulative changes\n",
    "\n",
    "# var_name = 'elev_med'\n",
    "# var_name = 'slope'\n",
    "var_name = 'aspect'\n",
    "\n",
    "if var_name in ('elev_med', 'slope'):\n",
    "    if var_name == 'elev_med':\n",
    "        x_label = 'Glacier median elevation (m)'\n",
    "        steps = [2900, 3050, 3200]\n",
    "    else:\n",
    "        x_label = 'Slope (°)'\n",
    "        steps = [18, 23, 28]\n",
    "    all_groups = []\n",
    "    for i in range(len(steps) + 1):\n",
    "        if i == 0:\n",
    "            idx = plot_df[var_name] < steps[i]\n",
    "            crt_group = f\"<{steps[i]}\"\n",
    "        elif i == len(steps):\n",
    "            idx = plot_df[var_name] > steps[i - 1]\n",
    "            crt_group = f\">{steps[i - 1]}\"\n",
    "        else:\n",
    "            idx = (plot_df[var_name] >= steps[i - 1]) & (plot_df[var_name] < steps[i])\n",
    "            crt_group = f\"{steps[i - 1]}\\n-\\n{steps[i]}\"\n",
    "        print(i, sum(idx))\n",
    "        plot_df.loc[idx, 'group'] = crt_group\n",
    "        if crt_group not in all_groups:\n",
    "            all_groups.append(crt_group)\n",
    "elif var_name == 'aspect':\n",
    "    all_groups = ['N', 'E', 'S', 'W']\n",
    "    x_label = 'Aspect'\n",
    "    for v in all_groups:\n",
    "        if v == 'N':\n",
    "            idx = (plot_df.aspect <= 45) | (plot_df.aspect > 315)\n",
    "        elif v == 'E':\n",
    "            idx = (plot_df.aspect > 45) & (plot_df.aspect <= 135)\n",
    "        elif v == 'S':\n",
    "            idx = (plot_df.aspect > 135) & (plot_df.aspect <= 225)\n",
    "        else:\n",
    "            idx = (plot_df.aspect > 225) & (plot_df.aspect <= 315)\n",
    "        plot_df.loc[idx, 'group'] = v\n",
    "\n",
    "plot_df.group = pd.Categorical(plot_df.group, categories=all_groups)\n",
    "\n",
    "# Calculate group means and round to 2 decimal places\n",
    "mean_data = pd.DataFrame(plot_df.groupby('group', observed=True).area_rate_prc.mean() * 100).reset_index()\n",
    "mean_data.columns = ['group', 'area_rate_prc']\n",
    "\n",
    "# Prepare text annotations\n",
    "plot_df_text = plot_df.groupby('group', observed=True).agg(\n",
    "    {'year_t0': 'count', 'area_inv': 'sum', 'area_rate_prc': 'min'}).reset_index()\n",
    "plot_df_text['area_total_prc'] = plot_df_text.area_inv / df_rates.area_inv.sum() * 100  # %A w.r.t. the total inv\n",
    "plot_df_text['y_pos'] = plot_df_text.area_rate_prc * 100 * 0.75\n",
    "plot_df_text['desc_n'] = plot_df_text.apply(lambda r: f'n = {r.year_t0}', axis=1)\n",
    "plot_df_text['desc_area'] = plot_df_text.apply(lambda r: rf'A_{{\\%}} = {r.area_total_prc:.1f}\\%', axis=1)\n",
    "\n",
    "# Calculate y-axis limits\n",
    "y_min, y_max = -35, 0\n",
    "\n",
    "# Create the plot\n",
    "g = (\n",
    "        p9.ggplot(mean_data, p9.aes(x='group', y='area_rate_prc')) +\n",
    "        p9.geom_bar(stat='identity') +\n",
    "        p9.geom_text(plot_df_text, p9.aes(x='group', label='desc_n'), y=y_min * 0.875, va='top', size=10, parse=True) +\n",
    "        p9.geom_text(plot_df_text, p9.aes(x='group', label='desc_area'), y=y_min * 0.95, va='top', size=9,\n",
    "                     parse=True) +\n",
    "        p9.labs(\n",
    "            y=r'2023-2015 relative area change (%)',\n",
    "            x=x_label\n",
    "        ) +\n",
    "        p9.ylim(y_min, y_max) +\n",
    "        p9.theme(\n",
    "            figure_size=(3.5, 4),\n",
    "            dpi=200,\n",
    "            legend_direction='horizontal',\n",
    "            legend_position='bottom',\n",
    "            text=p9.element_text(family='sans-serif', size=12)\n",
    "        )\n",
    ")\n",
    "display(g)"
   ],
   "id": "faa25b6187ca804a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Figure: overlays for both 2015 and 2023 (using the manually labelled glaciers)",
   "id": "4aa3a443a7a2b4ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "gl_df_inv = gpd.read_file(outlines_fp)",
   "id": "88f63af637e5554f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "entry_id = 'g_0498'\n",
    "r_gl = gl_df_inv[gl_df_inv.entry_id == entry_id].iloc[0]\n",
    "print(f\"Glacier ID: {r_gl.entry_id.replace('g_', '')}\\n({r_gl.Country} - {r_gl.LAT:.2f}° N, {r_gl.LON:.2f}° E)\\n\")"
   ],
   "id": "9c6100d4aaaec7ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# entry_id = 'g_0708'\n",
    "# entry_id = 'g_0731'\n",
    "# entry_id = 'g_0797'\n",
    "# entry_id = 'g_1006'\n",
    "entry_id = 'g_0954'\n",
    "\n",
    "# read the raster data (for the inventory year and 2023)\n",
    "year = 'inv'\n",
    "# year = '2023'\n",
    "\n",
    "fp_list = list(Path(f'../data/external/wd/s2_alps_plus/{year}/glacier_wide_sel/{entry_id}').glob('*.nc'))\n",
    "assert len(fp_list) == 1\n",
    "fp = fp_list[0]\n",
    "\n",
    "nc = xr.open_dataset(fp, decode_coords='all')\n",
    "\n",
    "# zoom in\n",
    "# nc = nc.isel(x=slice(50, -50), y=slice(100, -100))\n",
    "nc = nc.isel(x=slice(50, -50), y=slice(50, -50))\n",
    "\n",
    "# read the ensemble predictions\n",
    "if year == '2023':\n",
    "    gl_df = gpd.read_file('../data/outlines/manual_delineation/preds_2023_corrected.shp')\n",
    "    gl_df_pred = gpd.read_file(\n",
    "        model_root_dir / 'gdf_all_splits/seed_all/version_0/s2_alps_plus/2023/2023_preds_calib.shp')\n",
    "else:\n",
    "    gl_df = gpd.read_file(outlines_fp)\n",
    "    gl_df_pred = gpd.read_file(\n",
    "        model_root_dir / 'gdf_all_splits/seed_all/version_0/s2_alps_plus/inv/inv_preds_calib.shp')\n",
    "\n",
    "# reproject to the local UTM\n",
    "gl_df_proj = gl_df.to_crs(nc.rio.crs)\n",
    "gl_df_pred = gl_df_pred.to_crs(nc.rio.crs)"
   ],
   "id": "68b7bb617f9878e9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# gl_df_proj[gl_df_proj.entry_id.isin(\n",
    "#     ['g_0042', 'g_0042', 'g_0597', 'g_1660', 'g_0692', 'g_0708', 'g_0731', 'g_0791', 'g_0797', 'g_0491', 'g_1006'])]"
   ],
   "id": "3b28d34603f9df4e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# plot the SWIR-NIR-R image\n",
    "fig = plt.figure(dpi=300, figsize=(5, 5))\n",
    "img1 = nc.band_data.isel(band=[nc.band_data.long_name.index(b) for b in ['SWIR', 'NIR', 'R']]).values.copy()\n",
    "img1 = img1.transpose(1, 2, 0)\n",
    "\n",
    "img1 = nc.band_data.isel(band=[nc.band_data.long_name.index(b) for b in ['R', 'G', 'B']]).values.copy()\n",
    "img1 = img1.transpose(1, 2, 0)\n",
    "\n",
    "# clip and scale the image\n",
    "q = 0.03\n",
    "vmin = np.quantile(img1, q)\n",
    "vmax = np.quantile(img1, 1 - q)\n",
    "img1 = np.clip(img1, vmin, vmax)\n",
    "img1 = (img1 - vmin) / (vmax - vmin)\n",
    "\n",
    "extent = [nc.x.min(), nc.x.max(), nc.y.min(), nc.y.max()]\n",
    "plt.imshow(img1, extent=extent)\n",
    "# plt.grid(alpha=0.1, linestyle='--')\n",
    "plt.grid(False)\n",
    "plt.xlabel('Easting [m]')\n",
    "plt.ylabel('Northing [m]')\n",
    "\n",
    "# plot the glacier outlines\n",
    "gl_df_proj[gl_df_proj.entry_id == entry_id].plot(\n",
    "    ax=plt.gca(), facecolor='none', edgecolor='C2', linewidth=1\n",
    ")\n",
    "\n",
    "# plot the ensemble prediction\n",
    "gl_df_pred[gl_df_pred.entry_id == entry_id].plot(\n",
    "    ax=plt.gca(), facecolor='none', edgecolor='C1', linewidth=1\n",
    ")\n",
    "\n",
    "show_labels_on_first_only = False\n",
    "\n",
    "if not show_labels_on_first_only or entry_id == 'g_0708':\n",
    "    # build the legend (manually; default fails with UserWarning: Legend does not support handles for PatchCollection instances)\n",
    "    # y = gl_df_proj[gl_df_proj.entry_id == entry_id].date_inv.iloc[0].year\n",
    "    y = fp.stem[:4]\n",
    "    legend_handles = [\n",
    "        matplotlib.lines.Line2D([0], [0], color='C2',\n",
    "                                label=f'Reference ({y})' if year == '2023' else f'Inventory ({y})'),\n",
    "        matplotlib.lines.Line2D([0], [0], color='C1', label=f'Prediction ({y})'),\n",
    "    ]\n",
    "    # Add the custom legend to the plot\n",
    "    plt.legend(handles=legend_handles, loc='upper right')\n",
    "\n",
    "r_gl = gl_df_inv[gl_df_inv.entry_id == entry_id].iloc[0]\n",
    "print(f\"Glacier ID: {r_gl.entry_id.replace('g_', '')}\\n({r_gl.Country} - {r_gl.LAT:.2f}° N, {r_gl.LON:.2f}° E)\\n\")\n",
    "\n",
    "plt.gca().add_artist(\n",
    "    ScaleBar(\n",
    "        dx=1.0,\n",
    "        length_fraction=0.2,\n",
    "        font_properties={'size': 12},\n",
    "        frameon=True,\n",
    "        scale_loc='right',\n",
    "        location='lower right'\n",
    "    )\n",
    ")\n",
    "\n",
    "# disable the axis labels\n",
    "plt.axis('off')\n",
    "\n",
    "# plt.savefig(plots_dir / f\"{entry_id}_{year}.pdf\", bbox_inches='tight', dpi=300)\n",
    "\n",
    "plt.show()"
   ],
   "id": "2cf2de919a6247cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Figure: overlays for 2015 for the SGI glaciers used in the round-robin experiment",
   "id": "dab4ff011272b7b6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "entry_id_sel = [\n",
    "    'B35-02',\n",
    "    'A12d-10',\n",
    "    'E35-19',\n",
    "    'B51-12',\n",
    "    'B72-08',\n",
    "    'A54e-13',\n",
    "    'B90-04',\n",
    "    'C05-02',\n",
    "    'B22-01',\n",
    "    'A51f-10',\n",
    "    'E23-06',\n",
    "    'B63-05',\n",
    "    'A54g-11'\n",
    "]"
   ],
   "id": "74c8d3fce5b30d85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# read the raster data (for the inventory year and 2023)\n",
    "year = 'inv'\n",
    "\n",
    "for entry_id in sorted(entry_id_sel):\n",
    "    fp_list = list(Path(f'../data/external/wd/s2_sgi/{year}/glacier_wide_sel/{entry_id}').glob('*.nc'))\n",
    "    assert len(fp_list) == 1\n",
    "    fp = fp_list[0]\n",
    "\n",
    "    nc = xr.open_dataset(fp, decode_coords='all')\n",
    "\n",
    "    # zoom in\n",
    "    if entry_id == 'A12d-10':\n",
    "        nc = nc.isel(x=slice(50, -50), y=slice(50, -50))\n",
    "    else:\n",
    "        nc = nc.isel(x=slice(50, -50), y=slice(100, -100))\n",
    "\n",
    "    # read the ensemble predictions\n",
    "    gl_df_inv = gpd.read_file('../data/outlines/sgi2016/SGI_2016_glaciers.shp')\n",
    "    gl_df = gpd.read_file('../data/outlines/sgi2016/SGI_2016_glaciers.shp')\n",
    "    gl_df_pred = gpd.read_file(model_root_dir / 'gdf_all_splits/seed_all/version_0/s2_sgi/inv/inv_preds_calib.shp')\n",
    "\n",
    "    # reproject to the local UTM\n",
    "    gl_df_proj = gl_df.to_crs(nc.rio.crs)\n",
    "    gl_df_pred = gl_df_pred.to_crs(nc.rio.crs)\n",
    "\n",
    "    # plot the SWIR-NIR-R image\n",
    "    fig = plt.figure(dpi=300, figsize=(5, 5))\n",
    "    img1 = nc.band_data.isel(band=[nc.band_data.long_name.index(b) for b in ['SWIR', 'NIR', 'R']]).values.copy()\n",
    "    img1 = img1.transpose(1, 2, 0)\n",
    "\n",
    "    # clip and scale the image\n",
    "    q = 0.03\n",
    "    vmin = np.quantile(img1, q)\n",
    "    vmax = np.quantile(img1, 1 - q)\n",
    "    img1 = np.clip(img1, vmin, vmax)\n",
    "    img1 = (img1 - vmin) / (vmax - vmin)\n",
    "\n",
    "    extent = [nc.x.min(), nc.x.max(), nc.y.min(), nc.y.max()]\n",
    "    plt.imshow(img1, extent=extent)\n",
    "    # plt.grid(alpha=0.1, linestyle='--')\n",
    "    plt.grid(False)\n",
    "    plt.xlabel('Easting [m]')\n",
    "    plt.ylabel('Northing [m]')\n",
    "\n",
    "    # plot the glacier outlines\n",
    "    gl_df_proj[gl_df_proj.entry_id == entry_id].plot(\n",
    "        ax=plt.gca(), facecolor='none', edgecolor='C2', linewidth=1\n",
    "    )\n",
    "\n",
    "    # plot the ensemble prediction\n",
    "    gl_df_pred[gl_df_pred.entry_id == entry_id].plot(\n",
    "        ax=plt.gca(), facecolor='none', edgecolor='C1', linewidth=1\n",
    "    )\n",
    "\n",
    "    if entry_id == 'A12d-10':\n",
    "        # build the legend (manually; default fails with UserWarning: Legend does not support handles for PatchCollection instances)\n",
    "        # y = gl_df_proj[gl_df_proj.entry_id == entry_id].date_inv.iloc[0].year\n",
    "        y = fp.stem[:4]\n",
    "        legend_handles = [\n",
    "            matplotlib.lines.Line2D([0], [0], color='C2', label=f'Inventory (SGI2016)'),\n",
    "            matplotlib.lines.Line2D([0], [0], color='C1', label=f'DL4GAM Prediction'),\n",
    "        ]\n",
    "        # Add the custom legend to the plot\n",
    "        plt.legend(handles=legend_handles, loc='upper right')\n",
    "\n",
    "    r_gl = gl_df_inv[gl_df_inv.entry_id == entry_id].iloc[0]\n",
    "    # print(f\"Glacier ID: {r_gl.entry_id.replace('g_', '')}\\n({r_gl.Country} - {r_gl.LAT:.2f}° N, {r_gl.LON:.2f}° E)\\n\")\n",
    "\n",
    "    plt.gca().add_artist(\n",
    "        ScaleBar(\n",
    "            dx=1.0,\n",
    "            length_fraction=0.2,\n",
    "            font_properties={'size': 12},\n",
    "            frameon=True,\n",
    "            scale_loc='right',\n",
    "            location='lower right'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # disable the axis labels\n",
    "    plt.axis('off')\n",
    "\n",
    "    fp_out = plots_dir / f\"sgi_{entry_id}_{year}.pdf\"\n",
    "    plt.savefig(fp_out, bbox_inches='tight', dpi=300)\n",
    "    print(f\"Plot saved to {fp_out}\")\n",
    "\n",
    "    # plt.show()\n",
    "    # break"
   ],
   "id": "d9bc7e72993e93d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
